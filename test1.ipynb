{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the LLM with the API key\n",
    "llm = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Function to load the transcript from a JSON file\n",
    "def load_transcript(transcript_file):\n",
    "    with open(transcript_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    transcript_text = \"\"\n",
    "    for channel in data['results']['channels']:\n",
    "        for alternative in channel['alternatives']:\n",
    "            transcript_text += alternative['transcript'] + \" \"\n",
    "    \n",
    "    metadata = {\n",
    "        \"video_id\": data['metadata']['sha256'],\n",
    "        \"title\": data['metadata'].get('title', 'Unknown Title'),\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"text\": transcript_text.strip(),\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "\n",
    "# Function to chunk the transcript using a RecursiveCharacterTextSplitter\n",
    "def chunk_transcript(transcript, max_chunk_size=1000, overlap=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len,\n",
    "        keep_separator=True,\n",
    "        separators=['.', '؟', '!', '،', '\\n'] \n",
    "    )\n",
    "    chunks = text_splitter.split_text(transcript['text'])\n",
    "    \n",
    "    chunked_data = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_data.append({\n",
    "            \"chunk_id\": f\"{transcript['metadata']['video_id']}_{i}\",\n",
    "            \"text\": chunk\n",
    "        })\n",
    "    return chunked_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunks_to_file(chunked_data, output_file):\n",
    "    output_dir = os.path.join(\"E:\\\\ML and Data Science work\\\\Challenge\\\\datawars-llm-challenges\\\\Chunks\", chunked_data[0][\"chunk_id\"].split('_')[0])\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    output_file_path = os.path.join(output_dir, os.path.basename(output_file))\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        json.dump(chunked_data, file, indent=4)\n",
    "    \n",
    "    print(f\"Chunks saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_chunks(chunks_dir):\n",
    "    combined_chunks = []\n",
    "    for folder_name in os.listdir(chunks_dir):\n",
    "        folder_path = os.path.join(chunks_dir, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.startswith('chunked_') and file_name.endswith('.json'):\n",
    "                    file_path = os.path.join(folder_path, file_name)\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        chunks = json.load(file)\n",
    "                        combined_chunks.extend(chunks)\n",
    "    return combined_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\6417f2648d1cf2ee83a5ffaca82bbd60e289d62069c78e3a195816c1530790df\\chunked_DCDe29sIKcE.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\55a3448fbe97406fc667959b350b0c6b4d38afbe1de9e5bef56c471bc2c1ce0f\\chunked_Ercd-Ip5PfQ.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\5ef5aa2dcb3fdc44663f30bb71fd0c84ea582477bfbccb6f3e0fb4ba227f2c4b\\chunked_HQ6XO9eT-fc.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\2f8e35742eda12bdb3310f0ce74751429a280bab561b2f9ce6691910876c8a72\\chunked_KdmPHEnPJPs.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\8e045af20f5437f5159cbd38f71ed138cba8cafbc1ff340ca9c19d6f382d3161\\chunked_Lw2rlcxScZY.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\03eb1dcf661c916eb29c469b444d299bc464288c79db9d0aa4ad9db3aa330be4\\chunked_MPiz50TsyF0.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\39e73898b816b052c2aeb5d499afa08ada3ca3594777bf489c3fc8497dd85015\\chunked_N6hyN6BW6ao.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\cab0a1f43a4eba3c723e9bf39a3e576c2b6cc3ba748c961f17052b2887d4914a\\chunked_nKxLfUrkLE8.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\d810b9220e98fb751f49262ba6bcb8c62f18a41234634598ea38b83c6eeb510f\\chunked_T11QYVfZoD0.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\90912b0487bb8ee8331333237b0103d9dc06f1c317ff932fcdc9f44b1a76d489\\chunked_txMdrV1Ut64.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\a6eb411aa8da7a22533192ec6e330e1622de898a359ca14892da172cf0db8f9e\\chunked_UFuo7EHI8zc.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\3418e3c52525843e1245ef4c416f3e68f9c9b1f19730d92d5796e05b95ab820c\\chunked_UO98lJQ3QGI.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\df5fcb010e0ed106b60e3c01f16cf6460d766f9cc31f480040e6f59ad714e7ef\\chunked_W9XjRYFkkyw.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\32fb15ae61bdd9c4439c317446f3ee941f72e7bce4f6029f42b484b0f925da33\\chunked_x0Uguu7gqgk.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\409c5f69bcdc5d588873271498ec3753b0df2d515a2db67ac174b65f57cb7564\\chunked_XDv6T4a0RNc.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\1b0f7d892dc0a96c502daedc2474b08ac7f4fc4cbf0147dde2590baedca30da6\\chunked_XFZRVnP-MTU.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\dbb03ae717e6b61f969fdf2ff7f34e2f5e2cf2d1a1c3bda139e30f1805a3b1ab\\chunked_xN-Supd4H38.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\ebdcb0e7f8fe49ce424b530758a9309a47e97cae00443e134c39e401df5ea8e6\\chunked_zmdjNSmRXF4.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\7b4af4ca8448bcf6d8b60e1adfa5b17727a1564e1c775eb0aeb158dc7efb14af\\chunked_ZyhVh-qRZPA.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\4d3deb52bdb812b8a00769efb4f349802a4b7d5b3622fd4aec70138fc2bfb447\\chunked_zZZ_RCwp49g.json\n",
      "Chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\03245838c2dcb6e9fe6ddadbffcba6ccfe0824564a4fcf5fadf9396ad5b1f15c\\chunked__LWjaAiKaf8.json\n"
     ]
    }
   ],
   "source": [
    "videos_directory = \"E:\\\\ML and Data Science work\\\\Challenge\\\\datawars-llm-challenges\\\\transcripts\\\\videos\"\n",
    "chunks_directory = \"E:\\\\ML and Data Science work\\\\Challenge\\\\datawars-llm-challenges\\\\Chunks\"\n",
    "\n",
    "# Step 1: Process each transcript in the videos directory and chunk it\n",
    "for video_folder in os.listdir(videos_directory):\n",
    "    video_path = os.path.join(videos_directory, video_folder)\n",
    "    transcript_path = os.path.join(video_path, \"transcript.json\")\n",
    "    \n",
    "    if os.path.isfile(transcript_path):\n",
    "        # Load and chunk the transcript\n",
    "        transcript = load_transcript(transcript_path)\n",
    "        chunked_data = chunk_transcript(transcript, max_chunk_size=500, overlap=50)\n",
    "        \n",
    "        # Save the chunked data to the chunks directory\n",
    "        output_filename = f\"chunked_{video_folder}.json\"\n",
    "        save_chunks_to_file(chunked_data, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined chunks saved to: E:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\combined_chunks.json\n"
     ]
    }
   ],
   "source": [
    "combined_chunks_output = \"E:\\\\ML and Data Science work\\\\Challenge\\\\datawars-llm-challenges\\\\combined_chunks.json\"\n",
    "\n",
    "# Step 2: Combine all the chunks into a single JSON file\n",
    "combined_chunks = combine_chunks(chunks_directory)\n",
    "with open(combined_chunks_output, \"w\") as outfile:\n",
    "    json.dump(combined_chunks, outfile, indent=4)\n",
    "\n",
    "print(f\"Combined chunks saved to: {combined_chunks_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'chunk_id': '90912b0487bb8ee8331333237b0103d9dc06f1c317ff932fcdc9f44b1a76d489_33', 'text': \"a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read. So I am going to call this country group, and I'm just going to set this equal to this df.groupby. And now, instead of typing this every time, we can just reference this country group variable here. So\"}, page_content=\"a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read. So I am going to call this country group, and I'm just going to set this equal to this df.groupby. And now, instead of typing this every time, we can just reference this country group variable here. So\"), Document(metadata={'chunk_id': '90912b0487bb8ee8331333237b0103d9dc06f1c317ff932fcdc9f44b1a76d489_15', 'text': \". So first we're going to split the object, and then we're going to apply a function, and then it will combine those results. So first, let's look at splitting the object. Now, in this case, we want to group all of the results by country. So to do this, we can simply say, df.groupby, and then we will pass in, this is going to be a list of columns that we want to group on. And I'm just gonna pass in a single column here for country. So if I run this, then what we get back here is this data frame group by object. So what is this object, and what exactly can we do with this? So first, let's explain a bit what this is. So, this object contains a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read\"}, page_content=\". So first we're going to split the object, and then we're going to apply a function, and then it will combine those results. So first, let's look at splitting the object. Now, in this case, we want to group all of the results by country. So to do this, we can simply say, df.groupby, and then we will pass in, this is going to be a list of columns that we want to group on. And I'm just gonna pass in a single column here for country. So if I run this, then what we get back here is this data frame group by object. So what is this object, and what exactly can we do with this? So first, let's explain a bit what this is. So, this object contains a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read\"), Document(metadata={'chunk_id': '90912b0487bb8ee8331333237b0103d9dc06f1c317ff932fcdc9f44b1a76d489_15', 'text': \". So first we're going to split the object, and then we're going to apply a function, and then it will combine those results. So first, let's look at splitting the object. Now, in this case, we want to group all of the results by country. So to do this, we can simply say, df.groupby, and then we will pass in, this is going to be a list of columns that we want to group on. And I'm just gonna pass in a single column here for country. So if I run this, then what we get back here is this data frame group by object. So what is this object, and what exactly can we do with this? So first, let's explain a bit what this is. So, this object contains a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read\"}, page_content=\". So first we're going to split the object, and then we're going to apply a function, and then it will combine those results. So first, let's look at splitting the object. Now, in this case, we want to group all of the results by country. So to do this, we can simply say, df.groupby, and then we will pass in, this is going to be a list of columns that we want to group on. And I'm just gonna pass in a single column here for country. So if I run this, then what we get back here is this data frame group by object. So what is this object, and what exactly can we do with this? So first, let's explain a bit what this is. So, this object contains a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read\"), Document(metadata={'chunk_id': '90912b0487bb8ee8331333237b0103d9dc06f1c317ff932fcdc9f44b1a76d489_15', 'text': \". So first we're going to split the object, and then we're going to apply a function, and then it will combine those results. So first, let's look at splitting the object. Now, in this case, we want to group all of the results by country. So to do this, we can simply say, df.groupby, and then we will pass in, this is going to be a list of columns that we want to group on. And I'm just gonna pass in a single column here for country. So if I run this, then what we get back here is this data frame group by object. So what is this object, and what exactly can we do with this? So first, let's explain a bit what this is. So, this object contains a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read\"}, page_content=\". So first we're going to split the object, and then we're going to apply a function, and then it will combine those results. So first, let's look at splitting the object. Now, in this case, we want to group all of the results by country. So to do this, we can simply say, df.groupby, and then we will pass in, this is going to be a list of columns that we want to group on. And I'm just gonna pass in a single column here for country. So if I run this, then what we get back here is this data frame group by object. So what is this object, and what exactly can we do with this? So first, let's explain a bit what this is. So, this object contains a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read\")]\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import json\n",
    "\n",
    "\n",
    "# Initialize the OpenAI GPT-3 embedding model\n",
    "embedding = OpenAIEmbeddings(api_key=api_key)\n",
    "\n",
    "# Load the chunks\n",
    "with open(\"E:\\\\ML and Data Science work\\\\Challenge\\\\datawars-llm-challenges\\\\combined_chunks.json\", \"r\") as file:\n",
    "    chunks = json.load(file)\n",
    "\n",
    "# Convert chunks to Document objects\n",
    "documents = [Document(page_content=chunk[\"text\"], metadata=chunk) for chunk in chunks]\n",
    "\n",
    "# Create a new Chroma vector store from the documents\n",
    "vectorstore = Chroma.from_documents(documents, embedding=embedding)\n",
    "\n",
    "# You can now use the vectorstore as needed, for example:\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Use the retriever to answer a query\n",
    "response = retriever.get_relevant_documents(\"How can I group a DataFrame in Pandas?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Initialize the LLM with the API key\n",
    "llm = OpenAI(api_key=api_key)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the questions based on the provided context only.\n",
    "    Please provide the most accurate response based on the question.\n",
    "    <context>\n",
    "    {context}\n",
    "    <context>\n",
    "    Questions: {input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create the document chain using the prompt template\n",
    "document_chain = create_stuff_documents_chain(llm, prompt_template)\n",
    "\n",
    "# Set up the retriever with search parameters (e.g., returning top 5 documents)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create the retrieval chain\n",
    "chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Define the query\n",
    "query = \"How can I group a DataFrame in Pandas?\"\n",
    "\n",
    "# Stream the response\n",
    "response = chain.invoke({\"input\": query, \"question\": query})\n",
    "    \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read. So I am going to call this country group, and I'm just going to set this equal to this df.groupby. And now, instead of typing this every time, we can just reference this country group variable here. So' metadata={'chunk_id': '90912b0487bb8ee8331333237b0103d9dc06f1c317ff932fcdc9f44b1a76d489_33', 'text': \"a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read. So I am going to call this country group, and I'm just going to set this equal to this df.groupby. And now, instead of typing this every time, we can just reference this country group variable here. So\"}\n",
      "\n",
      "page_content='. So first we're going to split the object, and then we're going to apply a function, and then it will combine those results. So first, let's look at splitting the object. Now, in this case, we want to group all of the results by country. So to do this, we can simply say, df.groupby, and then we will pass in, this is going to be a list of columns that we want to group on. And I'm just gonna pass in a single column here for country. So if I run this, then what we get back here is this data frame group by object. So what is this object, and what exactly can we do with this? So first, let's explain a bit what this is. So, this object contains a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read' metadata={'chunk_id': '90912b0487bb8ee8331333237b0103d9dc06f1c317ff932fcdc9f44b1a76d489_15', 'text': \". So first we're going to split the object, and then we're going to apply a function, and then it will combine those results. So first, let's look at splitting the object. Now, in this case, we want to group all of the results by country. So to do this, we can simply say, df.groupby, and then we will pass in, this is going to be a list of columns that we want to group on. And I'm just gonna pass in a single column here for country. So if I run this, then what we get back here is this data frame group by object. So what is this object, and what exactly can we do with this? So first, let's explain a bit what this is. So, this object contains a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read\"}\n",
      "\n",
      "page_content='. So first we're going to split the object, and then we're going to apply a function, and then it will combine those results. So first, let's look at splitting the object. Now, in this case, we want to group all of the results by country. So to do this, we can simply say, df.groupby, and then we will pass in, this is going to be a list of columns that we want to group on. And I'm just gonna pass in a single column here for country. So if I run this, then what we get back here is this data frame group by object. So what is this object, and what exactly can we do with this? So first, let's explain a bit what this is. So, this object contains a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read' metadata={'chunk_id': '90912b0487bb8ee8331333237b0103d9dc06f1c317ff932fcdc9f44b1a76d489_15', 'text': \". So first we're going to split the object, and then we're going to apply a function, and then it will combine those results. So first, let's look at splitting the object. Now, in this case, we want to group all of the results by country. So to do this, we can simply say, df.groupby, and then we will pass in, this is going to be a list of columns that we want to group on. And I'm just gonna pass in a single column here for country. So if I run this, then what we get back here is this data frame group by object. So what is this object, and what exactly can we do with this? So first, let's explain a bit what this is. So, this object contains a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read\"}\n",
      "\n",
      "page_content='. So first we're going to split the object, and then we're going to apply a function, and then it will combine those results. So first, let's look at splitting the object. Now, in this case, we want to group all of the results by country. So to do this, we can simply say, df.groupby, and then we will pass in, this is going to be a list of columns that we want to group on. And I'm just gonna pass in a single column here for country. So if I run this, then what we get back here is this data frame group by object. So what is this object, and what exactly can we do with this? So first, let's explain a bit what this is. So, this object contains a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read' metadata={'chunk_id': '90912b0487bb8ee8331333237b0103d9dc06f1c317ff932fcdc9f44b1a76d489_15', 'text': \". So first we're going to split the object, and then we're going to apply a function, and then it will combine those results. So first, let's look at splitting the object. Now, in this case, we want to group all of the results by country. So to do this, we can simply say, df.groupby, and then we will pass in, this is going to be a list of columns that we want to group on. And I'm just gonna pass in a single column here for country. So if I run this, then what we get back here is this data frame group by object. So what is this object, and what exactly can we do with this? So first, let's explain a bit what this is. So, this object contains a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document in response[\"context\"]:\n",
    "    print(document)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
