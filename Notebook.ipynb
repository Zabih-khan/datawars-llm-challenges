{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘‰Import all Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘‰Load All the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(metadata_file):\n",
    "    with open(metadata_file, 'r') as file:\n",
    "        metadata = json.load(file)\n",
    "    return metadata\n",
    "\n",
    "def load_transcript(transcript_file, metadata):\n",
    "    with open(transcript_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    transcript_text = \"\"\n",
    "\n",
    "    for channel in data['results']['channels']:\n",
    "        for alternative in channel['alternatives']:\n",
    "            for word_info in alternative['words']:\n",
    "                word = word_info['punctuated_word']\n",
    "                transcript_text += word + \" \"\n",
    "    \n",
    "    video_id = data['metadata']['sha256']\n",
    "    title = metadata.get('title', 'Unknown Title')\n",
    "    \n",
    "    return {\n",
    "        \"text\": transcript_text.strip(),\n",
    "        \"metadata\": {\n",
    "            \"video_id\": video_id,\n",
    "            \"title\": title\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘‰Make chunks of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_transcript(transcript, max_chunk_size, overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        length_function=len,\n",
    "        separators=['.', ',', '\\n', '\\n\\n']\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(transcript['text'])\n",
    "    \n",
    "    chunked_data = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_data.append({\n",
    "            \"chunk_id\": f\"{transcript['metadata']['video_id']}_{i}\",\n",
    "            \"title\": transcript['metadata']['title'],\n",
    "            \"text\": chunk.strip()\n",
    "        })\n",
    "\n",
    "    return chunked_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘‰Now Save the Chunks into json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All chunks saved to: Chunks\\chunks.json\n"
     ]
    }
   ],
   "source": [
    "def save_chunks_to_file(combined_chunks, output_file):\n",
    "    output_dir = \"Chunks\"\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    output_file_path = os.path.join(output_dir, output_file)\n",
    "    \n",
    "    with open(output_file_path, 'w') as file:\n",
    "        json.dump(combined_chunks, file, indent=4)\n",
    "    \n",
    "    print(f\"All chunks saved to: {output_file_path}\")\n",
    "\n",
    "videos_directory = os.path.join(\"transcripts\", \"videos\")\n",
    "combined_chunks = []\n",
    "\n",
    "for video_folder in os.listdir(videos_directory):\n",
    "    video_path = os.path.join(videos_directory, video_folder)\n",
    "    transcript_path = os.path.join(video_path, \"transcript.json\")\n",
    "    metadata_path = os.path.join(video_path, \"metadata.json\")\n",
    "    \n",
    "    if os.path.isfile(transcript_path) and os.path.isfile(metadata_path):\n",
    "        metadata = load_metadata(metadata_path)\n",
    "        transcript = load_transcript(transcript_path, metadata)\n",
    "        chunked_data = chunk_transcript(transcript, max_chunk_size=800, overlap=100)\n",
    "        combined_chunks.extend(chunked_data)\n",
    "\n",
    "save_chunks_to_file(combined_chunks, \"chunks.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘‰Create Embeddings and store these into vector database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunks from: e:\\ML and Data Science work\\Challenge\\datawars-llm-challenges\\Chunks\\chunks.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding = OpenAIEmbeddings(api_key=api_key)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "chunk_file_path = os.path.join(current_dir, \"Chunks\", \"chunks.json\")\n",
    "print(f\"Loading chunks from: {chunk_file_path}\")\n",
    "\n",
    "# Load the chunks\n",
    "with open(chunk_file_path, \"r\") as file:\n",
    "    chunks = json.load(file)\n",
    "\n",
    "\n",
    "# Convert chunks to Document objects\n",
    "documents = [Document(page_content=chunk[\"text\"], metadata=chunk) for chunk in chunks]\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=documents, embedding=embedding, persist_directory='./chroma_db')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘‰Set up retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrive Relavent Documents\n",
      "Metadata:\n",
      "chunk_id: 90912b0487bb8ee8331333237b0103d9dc06f1c317ff932fcdc9f44b1a76d489_22\n",
      "text: . So, this object contains a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read. So I am going to call this country group, and I'm just going to set this equal to this df.groupby. And now, instead of typing this every time, we can just reference this country group variable here. So now let's take a look at one of these groups. So since we grouped our rows by country, then we can grab a specific group by country name. So I'll grab the group for the United States\n",
      "title: Python Pandas Tutorial (Part 8): Grouping and Aggregating - Analyzing and Exploring Your Data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Perform a retrieval\n",
    "response = retriever.invoke(\"HOW TO GROUP A DATAFRAME IN PANDAS?\")\n",
    "\n",
    "print(\"Retrive Relavent Documents\")\n",
    "for result in response:\n",
    "    print(\"Metadata:\")\n",
    "    for key, value in result.metadata.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As you can see it retrieve the relevant result from the chunks. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘‰Creating the LLM-powered RAG Chain for Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM with the API key\n",
    "llm = OpenAI(api_key=api_key)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "human\n",
    "\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {input} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘‰ Now Generate Response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To group a DataFrame in Pandas, you can use the groupby function. This function involves splitting the object, applying a function, and then combining the results. You can specify which column you want to group by, such as country, and then apply a specific function to the grouped data.\n",
      "     The groupby function will return a DataFrameGroupBy object, which allows you to perform various operations on the grouped data. This object can be reused by setting it as a variable for easier reference and readability.\n",
      "     To see specific results based on a certain column, you will need to group the data by that column. The groupby function is specifically designed for this purpose and is used in combination with other functions to split, apply, and combine the data. Referencing the Pandas documentation can provide more information on how this function works.\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I group a DataFrame in Pandas?\"\n",
    "\n",
    "response = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "# Print the response\n",
    "print(response['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
