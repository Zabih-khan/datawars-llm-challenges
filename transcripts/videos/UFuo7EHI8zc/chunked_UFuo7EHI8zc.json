[
    {
        "chunk_id": "0",
        "text": "Hey there. How's it going everybody? In this video, we're gonna be learning how to work with date and time series data within Pandas. Now there's a ton of interesting stuff that we can do with datetime data, and we'll be learning about that here. So we'll learn how to properly read in our data so that we can use datetime functionality. We'll also see how to filter by datetimes, how to group dates by resampling the timeframes, and we'll also take a look at doing some simple plotting with our time"
    },
    {
        "chunk_id": "1",
        "text": "look at doing some simple plotting with our time series data as well. Now I'd like to mention that we do have a sponsor for this series of videos, and that is Brilliant. So I really wanna thank Brilliant for sponsoring this series, and it would be great if you all could check them out using the link in the description section below and support the sponsors. And I'll talk more about their services in just a bit. So with that said, let's go ahead and get started. Okay. So first of all, I've been"
    },
    {
        "chunk_id": "2",
        "text": "and get started. Okay. So first of all, I've been using the Stack Overflow survey data for this entire series so far, but that dataset doesn't actually have any date or time series data, so I had to choose a different dataset for this video. I downloaded some historical cryptocurrency data that we can analyze for this video, and as usual I'm going to have links to download the data and the notebooks that I'm using in the description section below. So I've got my notebook opened up here where"
    },
    {
        "chunk_id": "3",
        "text": "So I've got my notebook opened up here where I'm reading in this CSV file of data, and, let's go ahead and take a look at what this looks like. So we can see here that I'm loading in this CSV file and I called this eth_oneh, and that's because this is historical data for Ethereum, which is a cryptocurrency, and I'm, and this data is broken down on 1 hour segments. So if we look down here at the head of this data, we can see that we have some columns here. The first one is a date column, and"
    },
    {
        "chunk_id": "4",
        "text": "columns here. The first one is a date column, and these are broken down by the hour. We also have some other information here like the symbols, the open and closing values for these hours, the highs and lows, and also the volume. So we can so all of this here is for, let's see, March 13th, and this is for 8 pm, 7 pm, 6 pm, and so on. Now remember, if you want to see more, information about your DataFrames, so for example how many rows and columns there are, I can run df. Shape, and we can see"
    },
    {
        "chunk_id": "5",
        "text": "there are, I can run df. Shape, and we can see that there are 23,000 rows here, almost 24,000. So a good bit of data for us to work with. Okay. So now let's actually get into working with daytime data. So we have this date column here, and it looks like this is just giving us every hour of the day. But right now, this isn't actually a datetime object. I can kinda tell this just because it's not in a format that datetimes usually display as, but if you want to be sure, you can always try running"
    },
    {
        "chunk_id": "6",
        "text": "you want to be sure, you can always try running a pandas data, pandas datetime method on this to see if it works. So let me just grab the 1st row of this DataFrame, and I'll grab that date value. So, and then I'll go ahead and try to run a datetime method. So to grab that first step value, I'm just going to say df.loc, and we can see here that the index is just 0 over here, so I'm just going to pass in a 0, and I want to grab that date column there. So if I run what we have now, then we can see"
    },
    {
        "chunk_id": "7",
        "text": "So if I run what we have now, then we can see that I've plucked out that first date. So now let's just try to run a date time method on this. So there's one method called day name that will give us the weekday that this date fell on. But if I run this now and I say, okay, dot day name for this value here, if I run this, then we can see that we get an error and it says, that a string object has no attribute day name. And that's because we are reading this in as a string currently. So how do we"
    },
    {
        "chunk_id": "8",
        "text": "this in as a string currently. So how do we convert this to a datetime? So there's a few different ways that we can do this, and we'll go over some of those here. Now if you wanna convert a column like we have here to a datetime, then we can use the pandas to underscore datetime method. So to do this, I could simply say we'll access that date column and we'll set this date column equal to and then we'll just say pd to underscore datetime. And now I want to pass in that same column to convert"
    },
    {
        "chunk_id": "9",
        "text": "now I want to pass in that same column to convert that to a datetime. Now I'm not going to run this right now because if I run this as is, then Pandas would do its best to figure out the formatting of the date time and convert it accordingly. But the date time that I have here is in a pretty different format, so I doubt that this is gonna work. But let's go ahead and try it out anyway. Okay. So I expected to get an error. If we scroll down and look at the error here, we can see that it says"
    },
    {
        "chunk_id": "10",
        "text": "look at the error here, we can see that it says unknown string format, so it did not know how to parse this date. But like I said before, depending on how your dates are formatted, then that might actually work for you. This just so happens to be formatted in a way that Pandas can't convert this automatically, without us telling it how our date is formatted. So what we need to do here is pass in a format string specifying how dates are formatted so that it can parse this correctly. Now I went"
    },
    {
        "chunk_id": "11",
        "text": "so that it can parse this correctly. Now I went ahead and I created the correct, string format ahead of time for this specific date, but just to be clear, I never really remember these formatting codes off the top of my head. I always need to go and find these codes within the pop, Python documentation. So I have that page open here, and I will leave a link to this in the description section as well. But however your date is formatted here, So ours started with the year, so we can see that that"
    },
    {
        "chunk_id": "12",
        "text": "started with the year, so we can see that that is a percent y. And then we have the month day, so we could find that in here. Another one is that we have, like, 8 PM and things like that. So we can see here that, these eyes here, this eye is for a 12 hour clock, which is what ours is doing, and then this percent sign p is for the local equivalent of AM or PM. So those are going to be in our format string, but I'll leave a link to this just in case your date formatting is different and you need"
    },
    {
        "chunk_id": "13",
        "text": "your date formatting is different and you need to create your own. So the format string that I need to pass in here, and again, this is basically just telling Pandas how to parse our date, we're gonna say that first we're gonna see the year, and then a dash, and then the month, and then the day with a dash in between that, and then a space, and then percent I was that 12 hour clock, and then there is a dash and then it is percent p. So let me go ahead and run this, and if I put this in"
    },
    {
        "chunk_id": "14",
        "text": "me go ahead and run this, and if I put this in correctly, then this should work. Okay. So we didn't get any, errors there, but let's go ahead and make sure. So I'm going to go ahead and look at the date column here, and we can see that now, these look more like datetime objects that we might be used to seeing in programming. So it converted 11 PM to 23. Well, I'm sorry. I thought 11 PM was the first one. No. It's 8 PM. Okay. So it, converted 8 PM to 20 and 7 PM to 19, and so on. And now that"
    },
    {
        "chunk_id": "15",
        "text": "PM to 20 and 7 PM to 19, and so on. And now that this is converted to a datetime, we should be able to run these datetime methods that gave us an error before. So up here where we got this error, where we tried to grab the day name for these, I'm just gonna copy that and paste that in down here. And now let's try to rerun this, and we can see that now it's saying that that first date in our series here, this, March 13th was a Friday. Okay. So that's nice. So it looked like it works. Now the way"
    },
    {
        "chunk_id": "16",
        "text": "nice. So it looked like it works. Now the way that we did this here is that we converted this to a date, after we loaded in our data with this line right here. But if we wanted to convert this to a date as we're loading in our data, then we can also do that as well. So if I go up here to the top where we loaded this in, at this read CSV line here, then I can actually pass in some arguments to read CSV, so that it loads in certain columns as date times, and then we can pass in our formatting"
    },
    {
        "chunk_id": "17",
        "text": "times, and then we can pass in our formatting string as well, so that it parses those as the data is read in. So to do this we need to pass in this parsed dates, argument here, and now I'm just going to pass in a list of the columns that are going to be dates. We only have one here, so it's just gonna be a list of 1 item. Oops. And I meant to put date, not dates. And now just like with before, if your dates are already formatted in a way that Pandas can parse them, then you don't need to add"
    },
    {
        "chunk_id": "18",
        "text": "Pandas can parse them, then you don't need to add anything else here. But we already saw before that we need to pass in a specific format. So to do this here, we can't just pass in a format string. We instead need to pass in a function that converts each String to a DateTime object. So first, let's create that function. And we've seen Lambda functions in this series before, but just in case you're unfamiliar with those, you can simply create a normal function instead if you are more comfortable"
    },
    {
        "chunk_id": "19",
        "text": "function instead if you are more comfortable with those, but this is just a shorter way. So to create this Lambda function, I'm just gonna call this d_ parser. I'm gonna set this equal to a Lambda function, and I'll just use x as the variable here. And now what do we want to return? So when we used pd.2 datetime down here, we actually passed in an entire series to pd.2 datetime. But now this is actually just going to be each individual string, and it's going to send each individual string"
    },
    {
        "chunk_id": "20",
        "text": "and it's going to send each individual string through this function. So in order to convert this, we can use a function called pd. Datetime. Strp time. That's how we convert a string to time. And then we can just pass in our string that we want converted to a datetime, and then the format. And I already had the format down here, so I'll just go ahead and copy that and paste that in here. And that's all we need for that date parser function. So now the argument for the date parser is date"
    },
    {
        "chunk_id": "21",
        "text": "So now the argument for the date parser is date underscore parser, and I'm going to set that equal to that d parser, variable there that is set to our Lambda function. Okay. So now if I run this cell here, then we can see that we didn't get any errors, so that's good. And now if I run this df.head here, then we can see that now our DataFrame, was already loaded in as a datetime. So we didn't have to do any conversions later on, it just did it as it was reading in that CSV file. Okay, so now"
    },
    {
        "chunk_id": "22",
        "text": "as it was reading in that CSV file. Okay, so now let's look at some more useful things that we can do with date times. So first, I'm going to delete the sales that we have below here, so that we are not converting these columns again, since they are already loaded in as, dates. So I'll delete that one. I will delete that one since that was what was converting it earlier. I'll delete that as well, And I'll keep this one here just for reference since I will have these up on my GitHub afterwards."
    },
    {
        "chunk_id": "23",
        "text": "I will have these up on my GitHub afterwards. Okay. So before, actually right here, we saw how to run a datetime method on a single value, when we use this day name method. But what if we want to run that method on our entire series? So let's say that we wanted to view the day name of this entire date column here. So to do this, we can access the dt class on the Series object and access the datetime methods that way. So to do this, we can just say we can first grab that series, so that date"
    },
    {
        "chunk_id": "24",
        "text": "say we can first grab that series, so that date column is going to return a series. If I run that, we can see that we get all those values. And now if we wanted to access the dt class on the series object, then we can just say dot dt, and now the date time method that we wanna use. So if I wanna get the day name of all these values, then I can just do day name there. And if I run that, then we can see that we get the day of the week for each of the dates in this Series. So using the dt class on"
    },
    {
        "chunk_id": "25",
        "text": "dates in this Series. So using the dt class on the Series object is very similar to how we access the string class or the str class for the string methods on an entire Series, and we saw that in previous videos. So this can definitely be pretty useful. So let's say that we wanted to, you know, create another column so that we could quickly reference what day all of these trades took place. So to do that, we could just grab what we have here, and I could simply create a new column by, simply"
    },
    {
        "chunk_id": "26",
        "text": "and I could simply create a new column by, simply like I'm accessing a column. So I could call this column day of week and set this equal to and paste in that date time method there. If I run this, and then we look at our data frame, then we can see that now we can quickly see over here on the right, that, okay, 13th was a Friday, and then we have these dates down here towards the end. This was a Saturday. So it's nice to see or be able to see what days these trades actually took place. So now"
    },
    {
        "chunk_id": "27",
        "text": "days these trades actually took place. So now let's look at how we can explore our data a bit. So we can see by looking at the indexes here on the far left that there are over 20,000 rows in this dataset. So let's see how we can view the earliest and latest dates in this data. So to do this we can use the min and max methods. So to see the earliest date, I could simply access this date series here and I could just run the min method on this. If I run that, then we can see that the earliest date"
    },
    {
        "chunk_id": "28",
        "text": "run that, then we can see that the earliest date that it gives us is 20 17, 0701. Now if I wanted to see so what is that? That's, July 1, 2017. So to view the most recent date that I have, and it should be the date that I downloaded this data, then I can just look at the max value here. And if I run this, then we can see that, this is March 13th 2020, which actually was the day that I downloaded this data. And one really cool thing with date times is that we can actually subtract dates in order"
    },
    {
        "chunk_id": "29",
        "text": "is that we can actually subtract dates in order to view the time between those two dates, and this is called a time delta. So to get the amount of time that spans between these two dates here, then I could simply say, take the max value and then subtract the min value. And if I run this, then we can see that we get this timedelta, that says that there were almost a 1000 days between the earliest date in our dataset and the most recent. So we have 986 days in this entire dataset of"
    },
    {
        "chunk_id": "30",
        "text": "So we have 986 days in this entire dataset of cryptocurrency data, almost a1000. So that would definitely be a lot of days to look through if we want to find some specific ranges. So what if we wanted to do some filters by date? So for example, let's say that we just wanted to view the data for 2020. Now that we have these converted to date times, we can create filters just like we have in previous videos, and we should be able to use, strings that are formatted like date times, or we can use"
    },
    {
        "chunk_id": "31",
        "text": "that are formatted like date times, or we can use actual date time objects. We'll take a look at both. So let's see an example of this in some code so that it makes some more sense. So first, I'm gonna create a filter in a separate variable like I've done in previous videos, but you can also do this inline if you prefer to do it that way. I just think that, creating our filters separate is a little bit easier to read. So let's say that I want, our date series. I want the objects or the rows"
    },
    {
        "chunk_id": "32",
        "text": "our date series. I want the objects or the rows that are greater than, and then I'm just gonna pass in a string here for now. And I can just pass in a 2020 there, and Pandas will know that I'm talking about the year 2020. Let's actually do a greater than or equal to here. Okay. So now that I have that filter, let's just do a df dotloc. Again, we've seen this in previous videos, and then I'll pass in that filter. So if I run this, then my bottom row here should be January 1, 2020, and it is. And"
    },
    {
        "chunk_id": "33",
        "text": "here should be January 1, 2020, and it is. And we can see that we have, 17000 hours here of 2020 data, or I'm sorry. That's, 1700 hours of 2020 data. Okay. So the reason that this doesn't go above 2020 is simply because, you know, our latest data runs out. So we're not getting 2021, since, 2021 hasn't happened yet. But what if we wanted data for 2019? Well, in order to do that, we'd also have to put in an upper bound as well. So to do that, I'm gonna say, okay. We want our data to be greater"
    },
    {
        "chunk_id": "34",
        "text": "gonna say, okay. We want our data to be greater than or equal to, 2019, and and we just wanna do an ampersand there. I'll go ahead and copy this here and then just replace this with a less than, and we'll say less than 2020. If I run this, then we can see that our bottom row here, we have January 1, 2019 at midnight, and then our top row here is December 31st at 11 pm, 2019. So that gives us all of the rows of data that we have for 2019. And right now we're just using, strings up here for these"
    },
    {
        "chunk_id": "35",
        "text": "now we're just using, strings up here for these comparisons, but we can use actual date times as well. So to do that, we could actually say I could just say pd.datetime, and then let me go ahead and pass in the month and day here as well. So I'll say that I want this to be greater than 2019, January 1st. And then I'll just grab this here and replace this 2020. And then I'll say but I want this to be less than 20 twenty's January 1st. So now if I run this, whoops, and I got an error here. The it"
    },
    {
        "chunk_id": "36",
        "text": "run this, whoops, and I got an error here. The it says, you know, integer is required. Got a string. That might not make sense. What I did here is I don't want pd.datetime. That was my mistake. I want to do the same thing that we did before and do to datetime so that it converts the string here to a datetime. So let's do pd.todatetime for both of those and run this. And now we can see that we get those same results as before, for all of the rows in 2019. Now one nice feature about dates is that"
    },
    {
        "chunk_id": "37",
        "text": "in 2019. Now one nice feature about dates is that if we set our index so that it uses, the date, which would actually be a good idea for this dataset since all of these, date or time stamps are unique, then we can actually do this same thing by using slicing instead. So let's see what this looks like so that it makes more sense. So first, let's set our index so that it's using this, date column here. So here at the bottom, I'm gonna say, df dot set underscore index, and then I'm gonna pass in,"
    },
    {
        "chunk_id": "38",
        "text": "set underscore index, and then I'm gonna pass in, that we wanna set the index to date. And if I run this, then that looks good. We have, set it our index to use date here. And now that that looks good, it actually didn't make that change. I want to say in place is equal to true, to make that change permanent. So I'll run that, and if we look at our data frame again, then now we have that date as our index. And now with that date index, we can actually filter our dates just by passing them into"
    },
    {
        "chunk_id": "39",
        "text": "filter our dates just by passing them into our brackets. So if we wanted the data for 2019, then I could literally just say, that I want the data here for 2019, pass that into my brackets. If I run that, then we can see that we get the same thing here. We get, this value for, January 1st, and then the top value here is for, December 31st. So it's a bit easier, to, you know, just access these within brackets when these are our indexes, rather than creating a filter. Now if you want to grab dates"
    },
    {
        "chunk_id": "40",
        "text": "creating a filter. Now if you want to grab dates for a specific range, then you can use a slice. So let's say that we wanted all of the data for January February of 2020. So to do that using this slicing here, then I could say, okay. I want from 2020 o 1, which would be January, and then I could just do a slice here using that colon, and then say, okay. Well, I wanna go up to, February of 2020. So if I run this, the second value here is inclusive, so we can see that we have, January 1, 2020"
    },
    {
        "chunk_id": "41",
        "text": "so we can see that we have, January 1, 2020 down here at the bottom. That slices all the way up to February 29th, since this was a leap year. Now this can be really useful for analyzing our data, because let's say that we wanted to get the average closing price for Ethereum for all of our rows of these dates. To do that we could simply grab this close column here and then grab that average or grab that mean. So to do that we can just say let me copy this part here. First let me just access that"
    },
    {
        "chunk_id": "42",
        "text": "this part here. First let me just access that close series there, that column. If I run that, then we can see that we get all of those closing values on each of those hours for all of those days. And now to get the mean of that, I can just say dot mean, and that gives us the average closing price for all of those rows within that time frame. And remember, each of those days is reporting by the hour. But what if we wanted to see this data in a different way? What if we instead wanted to look at"
    },
    {
        "chunk_id": "43",
        "text": "way? What if we instead wanted to look at this data on a daily basis instead of on an hourly basis? Well, first, we need to think about what would make sense to view on a daily basis. So for example, let's say that we wanted to, you know, view the highs for each day. So right now, we have, all of these highs broken down, by hour. Let me actually look at the first let me grab this date range here, and let's look at the first, 24 of these so that we can get 24 hours here. So we can see that for"
    },
    {
        "chunk_id": "44",
        "text": "we can get 24 hours here. So we can see that for February 29th, we have all these different hours here, and each hour has a different high value. But what if we were like, okay, well, we see all these different high values here, but what was the highest value of the day? So actually, let me just grab a single day here, and then we will look at the high values for that. So instead of doing all of these dates here, I'm just going to grab January 1, 2020, and then we will look at the high values"
    },
    {
        "chunk_id": "45",
        "text": "1, 2020, and then we will look at the high values for that day. So again, we don't really care what the highs are for each hour of each day, We just want to know the high for the entire day. So to do this, all we need to do is grab the max value for this series. And we saw how to do this. It's just like running mean right here. All we have to do is say dot max. And if I run that, then we can see that the high value for that day was 132 point 68. Okay. So let's remember this value here right"
    },
    {
        "chunk_id": "46",
        "text": "68. Okay. So let's remember this value here right now, this 132.68, because we're going to see how we can resample our data, so that we can get the highest trades for each day of our data, and then we'll use this one here to compare for, January 1, 2020. So again, right now our data is broken down on an hourly basis. So if we want to redo this so that it's instead broken down by day or week or month, then we'll do this by doing something called resampling. So let's see what this looks like. So"
    },
    {
        "chunk_id": "47",
        "text": "resampling. So let's see what this looks like. So if I want to resample this and see the high value by day, then I can simply access this high column here. And then on that series I can say, okay, I want to resample this. And now we have to tell resample how we want to resample this data. Right now it's hourly. If I put in a d, then it resamples it to days, and I can do 1 d or 2 d. You can do whatever there. You can do a w for week. There's all kinds of different codes here. Now, just like with"
    },
    {
        "chunk_id": "48",
        "text": "of different codes here. Now, just like with these datetime formats, I hardly ever remember these, so I always need to look them up in the documentation. So I've got this pulled up in the Pandas documentation here for these date offsets, and I will leave a link to this page in the description section below as well, if you all would like to try out some of these. But we can see we have hour, minute, second, milliseconds, microseconds, all kinds of things. If you're doing finances, you can do,"
    },
    {
        "chunk_id": "49",
        "text": "of things. If you're doing finances, you can do, quarterly and things like that. So I wanna do this on a daily basis, so I'm gonna put a d there. And now we have to tell it, okay, well, what do we want to, do with these resamplings? If I'm looking at entire days here, so if I take, this entire day of 1st, what do I want to do with this high value? And we're just saying, well, we want the max value for each of those days. So if I run this, then we can see that that gives us a series with all of"
    },
    {
        "chunk_id": "50",
        "text": "can see that that gives us a series with all of the high values for each day. So now let's save this series here as a new variable and look up the specific date that we used before. So I'm gonna save this as a variable and call that highs, and then let's access that specific date of 2020, 0101, for the highs. Now what we should get here, since we're using the same date that we did here, we should get this value of 132 point 68. So if I run that, then we can see that the high for that day, was"
    },
    {
        "chunk_id": "51",
        "text": "then we can see that the high for that day, was in fact, equal to what we did here. So that works. But now instead of just getting one day at a time like we did here, now that we've resampled this, now we have those high values for, every single day in our data. Okay. So why would something like this be useful? I mean, you know, that might be useful just because it's interesting, but there are other things that we can do as well. So let's say that maybe we wanted to plot this out. But instead"
    },
    {
        "chunk_id": "52",
        "text": "maybe we wanted to plot this out. But instead of, you know, viewing a plot that had these prices broken down hour by hour, now we can just do a plot for the total price broken down by day. So within Jupyter Notebooks, it's extremely easy to plot out information. I'm actually going to do an entire series on plotting with pandas, so I'm not going to go into a ton of details in this video, but we will see how we can do a you know, very simple line plot here. So to do this, we first need to use"
    },
    {
        "chunk_id": "53",
        "text": "plot here. So to do this, we first need to use this special line within Jupyter Notebooks that allows our plots to display within the browser. So all we have to do is say this is a percent sign here then we can say matplotlib inline. Now one thing that I do want to mention here is that I did have go and install matplotlib in the, virtual environment that I'm using. So if you've only installed, you know, pandas or and that's it, then you might want to go back and install matplotlib, or else you"
    },
    {
        "chunk_id": "54",
        "text": "to go back and install matplotlib, or else you will get an import error here. But I went and installed that in my, virtual environment, so we can see that that worked there. And with that one line of code there, now we can display plots directly within our Jupyter notebook. So I can simply run the plot method on this data frame variable that was resampled and get a plot of that. So I'm just gonna say, okay, I want highs plotted out. So highs dot plot. I'll run that, and we can see that we get a"
    },
    {
        "chunk_id": "55",
        "text": "plot. I'll run that, and we can see that we get a nice, matplotlib plot here. Okay, so that's pretty nice for, you know, just a few lines of code there. Now one thing that you might be wondering is if it's possible to resample multiple columns at once. And we can do that by running the resample method on our entire data frame instead of 1 a single series. So for example, what do I mean by this? Okay. So, whenever I say, you know, resample multiple columns at once, I mean that what if we wanted"
    },
    {
        "chunk_id": "56",
        "text": "columns at once, I mean that what if we wanted to resample this by day, but so far we've only seen, okay, how we got the high value. But what if we said, okay. Well, I wanna resample this by day, but I also want, you know, the average, closing cost of that entire day. I want the sum of all of these volumes for that entire day. And then I want the, you know, the max high value, and I want the min low value. So the way that we've done that down here, where we just access that single column, we"
    },
    {
        "chunk_id": "57",
        "text": "here, where we just access that single column, we wouldn't be able to do it using this method that we did here. So in order to resample and use multiple columns like that, here's how we can do this. So we can do this by running the resample method on our entire data frame. So if you want to use the same aggregation method on all of your columns so for example, let's say, df dot resample. So now we're resampling our entire DataFrame object here, and now we're gonna pass in what we wanna resample"
    },
    {
        "chunk_id": "58",
        "text": "now we're gonna pass in what we wanna resample on. Instead of day, let's change it up and do week. Now we'll resample by the each week. So if you want to use the same aggregation method on everything, then you can just put in that aggregation method there. So if I run this, then this is gonna give me the mean values for each of our columns on a weekly basis. Now this is cool that we can do this, and sometimes you might wanna do something like this. But in this instance, it doesn't really make"
    },
    {
        "chunk_id": "59",
        "text": "But in this instance, it doesn't really make sense to use mean to get the average of all of our columns. So for example, there's no real reason to get, you know, the average volume per hour or something like that. You probably wanna get the sum for the, you know, entire time period. Or for our high and low values here, these are giving us the average highs and the average lows, but the point of a high and low value is to know the high for that time period and the low for that time period. So we"
    },
    {
        "chunk_id": "60",
        "text": "period and the low for that time period. So we probably don't want mean here either. So how can we resample this, to where we can, you know, resample and use multiple columns, but also use multiple aggregation methods? Now we've actually seen this in previous videos and use this method, but what we want to use here is the agg, the agg method. And the agg method also accepts a map of columns and the aggregation functions that we want to run on that column. So for example, let's do this with the"
    },
    {
        "chunk_id": "61",
        "text": "column. So for example, let's do this with the values for, let's see. We'll do the closing column, we'll do the high and low columns, and then we'll also do the volume here. So I'm going to grab this from up here, and then we'll do df. Resample, and we'll pass in a w for a weekly basis. And now instead of passing in dot mean, like we did up here, I'm going to pass in dotagg. And now I can pass in a dictionary of the columns and, or the column names, and then the values will be the aggregation"
    },
    {
        "chunk_id": "62",
        "text": "and then the values will be the aggregation function that we want to use on that column. So for example, let's say that for the closing value, I do want to grab the mean of that, and then I'll say for the high column, I want to use the max, aggregation function for that since we want the max value. For the low column, I want to get the min. And for volume, I'll go ahead and just sum up, all of the volume for that entire time period. Okay. So, again, the keys here for this dictionary that we"
    },
    {
        "chunk_id": "63",
        "text": "again, the keys here for this dictionary that we passed in the ag, the ag method, this is the, column name here, then this is the aggregation function. So we're taking the mean of close, we're taking the max for this entire weekly period here, for the highs, the min for the low, and then sum for volume. So if we run this, then it gives us this nice weekly overview of the, you know, the weekly highs and the weekly lows here and also the average, closing costs here, and we also have the summation"
    },
    {
        "chunk_id": "64",
        "text": "costs here, and we also have the summation of the volume of trades. So, you know, this really touches on what we can do with date times and time series data in Pandas. Like I said a little bit ago, I do plan on doing a full series on pandas plotting where we'll cover more advanced topics, you know, such as plotting, plotting things out and having rolling averages for data and things like that. Now before we do end here, I do wanna thank the sponsor of this video, and that is Brilliant. And I"
    },
    {
        "chunk_id": "65",
        "text": "of this video, and that is Brilliant. And I really enjoy the tutorials that Brilliant provides and would definitely recommend checking them out. So in this series, we've been learning about Pandas and how to analyze data in Python, and Brilliant would be an excellent way to supplement what you learn here with their hands on courses. They have some excellent courses and lessons that do a deep dive on how to think about and analyze data correctly. For data analysis fundamentals, I would really"
    },
    {
        "chunk_id": "66",
        "text": "For data analysis fundamentals, I would really recommend checking out their statistics course, which shows you how to analyze graphs and determine significance in the data. And I would also recommend their machine learning course, which takes data analysis to a new level where you'll learn about the techniques being used that allow machines to make decisions where there's just too many variables for a human to consider. So to support my channel and learn more about Brilliant, you can go to"
    },
    {
        "chunk_id": "67",
        "text": "and learn more about Brilliant, you can go to brilliant.orgforward/cms to sign up for free. And also, the first 200 people that go to that link will get 20% off the annual premium subscription, and you can find that link in the description section below. Again, that's brilliant.orgforward/cms. Okay. So I think that's gonna do it for this Pandas video. I hope you feel like you got a good idea for how to work with date and time series data within Pandas. And like I said, there's a lot more that"
    },
    {
        "chunk_id": "68",
        "text": "Pandas. And like I said, there's a lot more that we can cover with datetime data, but I feel like what we did here should definitely provide you with the basics of being able to convert, analyze, and resample your data so that you can do the exact analysis that you need. Now in the next video, we're going to be learning how to read data in Pandas from different sources. So far in this series, we've only covered, CSV files, but we're going to learn how to read in data from Excel, from websites,"
    },
    {
        "chunk_id": "69",
        "text": "how to read in data from Excel, from websites, SQL databases, and a few more. So be sure to stick around for that. But if anyone has any questions about what will be covered in this video, feel free to ask in the comments section below, and I'll do my best to answer those. And if you enjoy these tutorials and would like to support them, then there are several ways you can do that. The easiest way is to simply like the video and give it a thumbs up, and also it's a huge help to share these"
    },
    {
        "chunk_id": "70",
        "text": "up, and also it's a huge help to share these videos with anyone who you think would find them useful. And if you have the means, you can contribute to Patreon, and there's a link to that page in the description section below. Be sure to subscribe for future videos, and thank you all for watching."
    }
]