[
    {
        "chunk_id": "chunk_0",
        "text": "Hey there. How's it going everybody? In this video, we're going to be learning how to handle missing values and also how to clean up our data a bit. Now, almost every dataset that you're going to be working with is likely going to have some missing data or data that we'd like to clean up or convert to a different data type. So we'll learn how to do all of that here. Now towards the end of the video, we'll combine what we learn here to be able to look at our Stack Overflow survey data and calculate the average years of experiences of developers who answer the survey. So be sure to stay around for that, and it's going to be great practice for what we learned here. Now, I would like to mention that we do have a sponsor for this series of videos, and that is Brilliant. So I really want to thank Brilliant for sponsoring the series, and it will be great if you all can check them out using the link in the description section below and support the sponsors, and I'll talk more about their services in just a bit. So with that said, let's go ahead and get started. Okay. So first, let's talk about how to drop missing values. So I have my snippets file open here, and we've seen this in previous videos. And again, if anyone wants to follow along, then I'll have a link to all of these notebooks and the data in the description section below. And as we've seen in previous videos, we'll learn how to do some of this in our smaller snippets data frame first, and then we'll see how to do some interesting stuff on our larger Stack Overflow dataset, to get this working on some real world data. So for this video, I've added some null values here into our snippets data frame, that we didn't have before. So I added some extra first names here, and we can see that I just have one that is, a numpy dot n a n, which is a not a number value. I also imported NumPy up here at the top. This and this one here is just a none value, and then I also have some custom missing values as well. This one is just a string of NA, and this one is just a string of missing. So I have some NANs, some nones, and stuff like that thrown throughout this data so that we have some missing values. So you're going to see this a lot, that when we work with Pandas, we're going to have some missing data. And depending on what it is you're trying to do, you might want to handle this in different ways. So one thing you might want to do with missing data is to simply remove it. So for our small data frame here, let's say that we're going to do some analysis with these people in the data frame, but if they don't have their",
        "video_title": "Python Pandas Tutorial (Part 9): Cleaning Data - Casting Datatypes and Handling Missing Values",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_1",
        "text": "and stuff like that thrown throughout this data so that we have some missing values. So you're going to see this a lot, that when we work with Pandas, we're going to have some missing data. And depending on what it is you're trying to do, you might want to handle this in different ways. So one thing you might want to do with missing data is to simply remove it. So for our small data frame here, let's say that we're going to do some analysis with these people in the data frame, but if they don't have their first name, last name, and email address, then we can't do what we're trying to do, so we'll just remove the rows that don't have those values. So in order to do this, we can use the drop in a method. So let's do this, and then I'll explain the results, and go over those. So all I'm going to do down here with my data frame is I'm going to say df.dropna, and we're going to run that without any arguments right now. So when we run this, we can see that now we only get 4 rows of data here. And up here we had, let's see, 4, 5, 6, 7. So we got these 4 rows here, because they didn't have any missing values. Now we do still have our bottom row here, which has some of our custom missing values, but we'll see how to deal with these in just a second. But for now, let's go over what drop NA is actually doing here. Now what's going on in the background is that drop na is using some default arguments. So I'm going to manually fill in these default arguments and it might make more sense why we got this specific result. So by default, I'll leave that one here, and now I'm gonna fill in drop in a again, but I'm gonna put the default arguments that it already has. And the default arguments of what this is doing in the background is it has an axis set to index, and it has a how variable set to any. So since this is what the method was using by default anyway, we should go ahead and get the same results here, and we can see that we do. We get the same results as we did when we ran this up here. But now let me actually explain these arguments here. So first, we have the axis argument. So this can either be set to index or set to columns. That is going to tell Pandas that we want to drop NA values when our rows are missing values when it is set here to index. If we set this to columns, then it would instead drop columns if they had missing values, and we'll look at that in just a second. Now the second argument here is how, we want to drop these. Or I guess a better way to frame that is this is",
        "video_title": "Python Pandas Tutorial (Part 9): Cleaning Data - Casting Datatypes and Handling Missing Values",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_2",
        "text": "actually explain these arguments here. So first, we have the axis argument. So this can either be set to index or set to columns. That is going to tell Pandas that we want to drop NA values when our rows are missing values when it is set here to index. If we set this to columns, then it would instead drop columns if they had missing values, and we'll look at that in just a second. Now the second argument here is how, we want to drop these. Or I guess a better way to frame that is this is the criteria that it uses for dropping a row or a column. So by default, this is set to any. So we're looking over our rows since this is set to index, and this is set to any here, so it will drop rows with any missing values. But this might not be what you want. Maybe, with this kind of analysis that we're doing, it's okay to have, you know, missing email or last name or something like that, but there just has to be something. It can't just be an entire row of missing values. So if that's the case, then we can instead change this how argument to all. And this will then only drop rows when all of the values in that row are missing. So now if I run this, then we can see that now we get back more rows than we did before because it kept the rows that had some missing values, but not all missing values. So we can see here we have an email missing, but there were some other, columns filled in. Now, we can see that everything was missing here, but they did have an email. So all of the values have to be missing in order for, this to actually drop those. So it looks like we are missing index of 4. If I go up to my original data frame here, we can see that that index had all missing values there. Okay. Now, if I instead change this axis to columns instead of index, then it will drop columns that have all missing values. We don't have any columns that have missing values all the way down, so it should just return our original DataFrame. So, if I say columns here and run this, then we can see that that's what we get because none of these columns have missing values all the way down. Now if I set this back to the default and drop columns with any missing values, then we'll actually get an empty dataframe returned because we have one row that is completely empty, that we saw here, this, index of 4. So for that row, each column is going to have at least one missing value. And if we set this to any, then any column, which is even a single missing value, will be dropped, which in this case is all of them. So if I change this to any, then",
        "video_title": "Python Pandas Tutorial (Part 9): Cleaning Data - Casting Datatypes and Handling Missing Values",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_3",
        "text": "columns have missing values all the way down. Now if I set this back to the default and drop columns with any missing values, then we'll actually get an empty dataframe returned because we have one row that is completely empty, that we saw here, this, index of 4. So for that row, each column is going to have at least one missing value. And if we set this to any, then any column, which is even a single missing value, will be dropped, which in this case is all of them. So if I change this to any, then since we have all missing values in one of these rows, that's just gonna give us an empty data frame. Now at this point, you might be wondering, okay. Well, my data is a bit more complicated than this, and I'm doing some analysis where I want to drop some missing values, but I only want to drop but I only want to drop rows that are missing values in a specific column. So for example, let's say that we're doing some analysis on our data, and it's fine if they don't have a first name or a last name, but we really need the email address. And if they don't have an email address, then we need to just drop those rows. So in order to do this, we can pass in a subset argument. So first, I'm gonna set our, axis here back to index so that we're dropping rows. And now, we want to pass in a subset argument. And this subset will be the column names that we are checking for missing values. So in this case, it's just going to be a single column. So I'm going to say subset is equal to, and I'm still going to pass in a list, even though this is just a single column, and I'll say email. So if I run this, then we can see that the data frame that we get back is full of rows that have, at least their email address filled in. And again, this one down here with these NA values, that is our custom missing values, and I'll show you how to treat those as missing values in just a bit. Now in this case here, since we're only passing in a single column for our subset, our how argument here, isn't really doing much because it's only going to look at the email address for missing values. So if an email address isn't filled in, then passing in either any or all for our argument here, would trigger that row to be removed. So even if I put this as all, it should give us the same results because we're only checking one value. But we can also pass in multiple columns to our subset. So what if we said, okay, well, in order for my data to be useful, I need either their last name or their email address, but I don't need both. So in order to do this, we",
        "video_title": "Python Pandas Tutorial (Part 9): Cleaning Data - Casting Datatypes and Handling Missing Values",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_4",
        "text": "email address for missing values. So if an email address isn't filled in, then passing in either any or all for our argument here, would trigger that row to be removed. So even if I put this as all, it should give us the same results because we're only checking one value. But we can also pass in multiple columns to our subset. So what if we said, okay, well, in order for my data to be useful, I need either their last name or their email address, but I don't need both. So in order to do this, we could just say, okay. They need, all of the values in last name and in email. Or I'm sorry there that, I got that reversed. They don't need their last name in the email. It just can't be that all of those values are missing. So as long as the last name or the email is there, then it shouldn't drop those rows. So if I run this, then we can see that we get some values that don't have an email, but they did have a last name. And also, we would get back some values that didn't have a last name but do have an email, just like this anonymous one here. It has an email, but it doesn't have a last name. And again, that's because we passed in all for our how argument, which means for a row to be dropped, both of the subset columns needed to be missing. Now, like we've seen in previous videos, this isn't permanently changing our data frame values. If we want to permanently change our data frame, then we'd have to add the in place argument and set that equal to true, here within this method. But we've seen that a bunch throughout the series so far, so I don't think I'll go over that again here. Okay. So now let's get to these custom missing values. We can see down here, that we have a row here that has some customized missing values. So for example, maybe the people who you got our data from, didn't know what to do with missing values, so instead, they just passed in a string of NA, or they passed in, you know, a string of missing, like we have here. So how would we actually handle these? Well, it depends on how we load in our data. In this case, we've created our data frame from scratch by, creating a dictionary and then creating our data frame here. So what we can do here is just simply replace those values with an NaN value. Now if we instead loaded in our data from a CSV file, then we could do something different. But first, I'll show this, and then we'll take a look at the CSV file later whenever I go over to the Stack Overflow data. So right here at the top where we created our data frame, I'm going to replace these values with a proper NumPy n a n value.",
        "video_title": "Python Pandas Tutorial (Part 9): Cleaning Data - Casting Datatypes and Handling Missing Values",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_5",
        "text": "data frame from scratch by, creating a dictionary and then creating our data frame here. So what we can do here is just simply replace those values with an NaN value. Now if we instead loaded in our data from a CSV file, then we could do something different. But first, I'll show this, and then we'll take a look at the CSV file later whenever I go over to the Stack Overflow data. So right here at the top where we created our data frame, I'm going to replace these values with a proper NumPy n a n value. So to do this, I'm just going to go a couple of lines down here, and we've seen this in previous videos, but we can use this replace here, and I'm replacing all the values in the entire data frame. So anytime we see a string of na, I'm going to replace that with numpy.na n, and again, I am importing numpy up here as MP. So that's where I'm getting, I'm able to use NumPy. And then I want to say in place equal to true because we actually want to modify that data frame. So if I run that, then that would replace those values, but I'm also gonna place, replace this string of missing as well with np.nanvalues, and I want to do that in place as well. So let's go ahead and run this. That should replace those values. And now, if I look at our data frame here, then we can see that we no longer have that string of missing or NA. These are now all NA values. And now, if we go back through and we run our cells where we dropped NA values, then these custom values should have been replaced, and it should treat those as missing values. So right here, we can see what our previous result was where we got this index of 6 with those custom values. If I rerun this now, we can see that that's gone. And the same with here, if I rerun this, then that is gone as well. Now if you don't actually want to make any changes, and we just want to see if certain values would or wouldn't be treated as NA values, then we could just run the NA or is NA method, and get a mask of values, as to whether or not these classify as NA or not. So let me just show you what I mean here. So I could say df. Is NA, and this is just going to give us a mask here of values that, are whether or not they are classified as an NA value. So we can see that our row 4 here, was all NA values, and so same thing with our row 6, and we can see some other missing values throughout here as well. Now sometimes, especially when we're working with numerical data, we might want to fill our NA values with So So for example, let's assume that we were",
        "video_title": "Python Pandas Tutorial (Part 9): Cleaning Data - Casting Datatypes and Handling Missing Values",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_6",
        "text": "me just show you what I mean here. So I could say df. Is NA, and this is just going to give us a mask here of values that, are whether or not they are classified as an NA value. So we can see that our row 4 here, was all NA values, and so same thing with our row 6, and we can see some other missing values throughout here as well. Now sometimes, especially when we're working with numerical data, we might want to fill our NA values with So So for example, let's assume that we were calculating grades for assignments or something like that, and you had some assignments that were NA because the student never turned in the assignment. Well, at that point, you could just decide if you wanted to score all missing assignments as zeros so that you could properly calculate up the grades. So to do something like this, we can use the fill in a method. So for example, I could say something like this. If I do a df.fillna, and then pass in a value, just to show you exactly what this is doing, I'm gonna fill all of our missing values with this capitalized missing string here. And if I run this, then we can see that all of those missing or all of those NA values were filled with this string capitalized as missing. Now, like I said before, I don't do this a lot with certain strings. I found this, to be most useful for numerical data, depending on how you're doing your calculations, but you might want to give n a values a value of 0 or negative one or something like that. So if it would make sense with your data and you had numerical data to replace your missing values with a 0, then you could just run df dot fillna 0. And if I go ahead and run this, then we can see that that works on our data frame as well. And again, just like with our other methods, if you want those changes to your data frame to be permanent and carry over into other cells, then simply just add that in place argument and set that to true to make that change permanent. Okay. So now let's look at another common thing that we're likely going to need to do with a lot of our data, and that is casting data types. So I have another column in my snippets here that I didn't have in previous videos. And I have up here, if we look, this is this age column. So let's say that we wanted to get the average age of all the people in this sample data frame. Well, right now, these might look like numbers when we print them out in our data frame down here, but the these are actually strings, and we can see this if we look at our data frame data types. So to do this, we can say dfdot d types, and that's",
        "video_title": "Python Pandas Tutorial (Part 9): Cleaning Data - Casting Datatypes and Handling Missing Values",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_7",
        "text": "So I have another column in my snippets here that I didn't have in previous videos. And I have up here, if we look, this is this age column. So let's say that we wanted to get the average age of all the people in this sample data frame. Well, right now, these might look like numbers when we print them out in our data frame down here, but the these are actually strings, and we can see this if we look at our data frame data types. So to do this, we can say dfdot d types, and that's not a method, it's just an attribute. So if I run this here, then we can see that it says all of these columns are objects. And when it says it's an object, it likely means it's a string or a mix of different things. So in the latest version of Python or Pandas, I'm sorry, they actually updated it so that there's actually a string data type now, but I'll do a video on those Pandas version updates at the end of this series since they actually released that updated version, as I was writing this course. But don't worry, there's not a lot that has changed to where, what you learned here will be outdated or anything like that. It's still mostly the same. But we can see here that our age column is a string because it's, this object data type. So if we wanted the average age, then it wouldn't work as it is now. So let's just see what this error looks like. So I'm going to grab the mean of that age column, and if I run this, then we can see that right now we get an error. And if I scroll down to see what this error was, it says can only concatenate, str not int to string. Now that might not be the most easy to understand error right there, but basically it's telling us that because that column is strings and not integers. So we need to convert that column to numbers instead of a string. Now there's a caveat, when doing this and this might throw some people off. So when we have n a n values in a column that we're trying to convert to numbers, then you need to use the float data type, and that's because the n a n value is actually a float under the hood. Let me go ahead and show this just to show you what this looks like. So I'm gonna look up the type of np.nan, and we can see that that is a float. So if we try to convert this column to integers, then it's going to throw an error when it runs into those n a n values because it can't convert those. So if I was to say, d f and of age is equal to, and now let's try to convert these to integers. So the way that you cast data types here is we can just",
        "video_title": "Python Pandas Tutorial (Part 9): Cleaning Data - Casting Datatypes and Handling Missing Values",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_8",
        "text": "Let me go ahead and show this just to show you what this looks like. So I'm gonna look up the type of np.nan, and we can see that that is a float. So if we try to convert this column to integers, then it's going to throw an error when it runs into those n a n values because it can't convert those. So if I was to say, d f and of age is equal to, and now let's try to convert these to integers. So the way that you cast data types here is we can just say, okay, I want the age column as type, and now we want to pass in the type that we want. If I try to convert these to integers, then this is going to give us an error because we have some NaN values. So we can see here, int argument must be a string, not none type. So when you're trying to convert these to numbers and you have those NaN values, you basically have 2 options here. If your column didn't have any missing values, then this would just work fine. We wouldn't even run into this error. But if it does have missing values, then you can either convert those missing values to something else, like a 0 using the fill in a method that we saw before, or you can just cast that column to a float instead. Now, I think this would be a bad idea to convert those missing values to a 0 or some other number, because we're trying to compute the average in this case. But depending on your data, that might be what you wanna do. But I'm gonna go ahead and just convert these to floats, so those NaN values stay missing values. So instead of an int here, I'm just gonna convert this to a float, and if I run this, then that seemed to have worked. So now, we can look at the data types again. So I'll say, d f whoops. Sorry. I wasn't typing in that cell. I can say, d f dot dtypes. And if we look at this, then we can see that now our age is a float object here. So now, let's see what happens when we try to take the average of that column. So I'll say df dot mean, and if I run that, then we can see that we get the average value for those ages. Now if you have an entire data frame of numbers or something like that that you are trying to convert all at once, then the data frame object has an as type method as well. So you could just say, df. As type, and then pass in whatever data type you're trying to cast everything to, and just convert everything in the DataFrame at once. But we have some mixed columns here, so we don't want to do that. Okay. So we've been looking at our small dataset right now to test this stuff out,",
        "video_title": "Python Pandas Tutorial (Part 9): Cleaning Data - Casting Datatypes and Handling Missing Values",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_9",
        "text": "average value for those ages. Now if you have an entire data frame of numbers or something like that that you are trying to convert all at once, then the data frame object has an as type method as well. So you could just say, df. As type, and then pass in whatever data type you're trying to cast everything to, and just convert everything in the DataFrame at once. But we have some mixed columns here, so we don't want to do that. Okay. So we've been looking at our small dataset right now to test this stuff out, but now let's take what we learned here and learn how this applies to real world data and do some analysis on our Stack Overflow survey data. Okay. So first of all, I mentioned earlier that if we had custom values for missing data, then it's a little bit easier to handle these when loading in a CSV. And what I'm talking about up here is up here at the top where we replaced these custom missing values. Let me show you how we would do this same thing, but loading in a CSV instead. So I'm gonna switch over here to my Stack Overflow survey data. Let me go ahead and rerun this just to make sure that, all of this stuff is running. Okay. So this notebook is still running. That's good. And again, this is that Stack Overflow data that we have been using throughout the series. And if you would like to follow along, then I do have a download link for this in the description section below. Okay. So if I wanted to ignore those custom values when loading in a CSV, then we can simply pass in an argument of a list of values that we want to be treated as missing. So here's how we would do this. If we had some custom missing values here in this CSV file, then I could simply create a list here of those missing values, and I will just call this NA Vals, and now I'll pass in a list of those. So let's say that we have some values that are a string of n a and a string of missing. So now what we could do here is just add in an argument and say n a values is equal to, and then that list that we just created. And if we run that, then we shouldn't get any errors. And when it reads in that CSV, then it will treat that list of values, as missing values and give them an NaN result. Now in this survey here, they did a good job of not having any weird occurrences like that, so that actually shouldn't change anything. Okay. So now, let's look at an interesting problem with casting some values. So let's say that for the developers who answered this survey, we wanted to calculate the average number of years of coding experience among all of them. Now, that might be a good thing to",
        "video_title": "Python Pandas Tutorial (Part 9): Cleaning Data - Casting Datatypes and Handling Missing Values",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_10",
        "text": "shouldn't get any errors. And when it reads in that CSV, then it will treat that list of values, as missing values and give them an NaN result. Now in this survey here, they did a good job of not having any weird occurrences like that, so that actually shouldn't change anything. Okay. So now, let's look at an interesting problem with casting some values. So let's say that for the developers who answered this survey, we wanted to calculate the average number of years of coding experience among all of them. Now, that might be a good thing to know to compare your experience against the average. But let's look at what this or why this might be difficult to calculate with this dataset. And us calculating this solution is actually going to apply several concepts that we've learned so far throughout this series. So the column to view the answer for, this question in the survey is called year's code. So let's look at some of these answers. So I'm just going to look at the top ten answers for year's code. So I will do a dot head, and let's look at the top ten. So if I run this, then at first glance, this doesn't really look like it'll be a problem. We just have a bunch of integers and the number of years that different respondents have been coding. So you might think that we can just grab the mean, of this column simply by saying, okay, if we just have a bunch of integers here and some NaN values, that's fine. Let's just grab the mean of that column. But if I run this, then we get an error. And if I scroll down here, then it says can only concatenate string, to string. And we saw this same error in our smaller dataset where the column was actually being loaded in as a string instead of numerical data, and we should know how to handle this by now since we did it in the smaller dataset. So let's try that. So let's try to convert this to floats and then take the average. So let me go back up here to the top where we ran this, and instead of running the mean here, I'm gonna say, okay. Well, let's convert this to a float so that we can grab that average. So I will say as type, and we wanna convert this to a float since there are NaN values. So if I run this, then we still get an error. So we didn't get an error in our smaller dataset here. So if I scroll down, then it says, could not convert string to float, and the string that it couldn't convert was less than 1 year. So this might be something that we didn't expect here. So obviously, we don't just have numbers and n a n values in this column. There is actually a string value that respondents could select that is equal to this string of less than 1 year",
        "video_title": "Python Pandas Tutorial (Part 9): Cleaning Data - Casting Datatypes and Handling Missing Values",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_11",
        "text": "float since there are NaN values. So if I run this, then we still get an error. So we didn't get an error in our smaller dataset here. So if I scroll down, then it says, could not convert string to float, and the string that it couldn't convert was less than 1 year. So this might be something that we didn't expect here. So obviously, we don't just have numbers and n a n values in this column. There is actually a string value that respondents could select that is equal to this string of less than 1 year for coding experience. So let's look at all the unique values of this of this column so that we can see exactly what's in here in case there are more strings like this, and I don't believe we've actually seen this in the series yet. Maybe we have. I can't really remember. But if we want to view unique values of a Series, then we can simply use the unique method. So we could also use the value counts method that we've seen several times before if we wanna count all the unique values, but we don't really wanna count them. We just wanna see all the unique values in this column. So to do this, I'm gonna say df and then access that years code column dot unique, Dot unique, that is a method. So if I run this whoops. And I spelled this wrong. Sorry. Having a hard time typing today. Okay. So if I run this, then this gives us all of the unique values of that column. And as we'd expect, there are a lot of numbers, but we see that, we also have some strings that are mixed throughout these numbers. Now we also have NaN values here, but we're not gonna worry about those. We just want to ignore the NaN values because that's just people who didn't answer the question. But we can see that the strings that we have throughout here are less than 1 year and more than 50 years of coding experience. Okay. So those are our only string values. So I am going to replace those with numbers so that we can get an idea of the average years people have been coding. So let's go ahead and replace less than 1 year here with a 0 since that's basically the same thing. If somebody has been coding for less than a year, then they've been coding for basically 0 years. So to do this, I can say df.yearscode and access that column, and then we can just replace that value of less than 1 year, and let's replace that with a 0. And we also want these to be in place equal to true, because we actually want to modify that data frame. So if I run that, then it should make that replacement. And now I'm also going to replace the value for more than 50 years here. And this is going to skew our results a bit depending",
        "video_title": "Python Pandas Tutorial (Part 9): Cleaning Data - Casting Datatypes and Handling Missing Values",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_12",
        "text": "a year, then they've been coding for basically 0 years. So to do this, I can say df.yearscode and access that column, and then we can just replace that value of less than 1 year, and let's replace that with a 0. And we also want these to be in place equal to true, because we actually want to modify that data frame. So if I run that, then it should make that replacement. And now I'm also going to replace the value for more than 50 years here. And this is going to skew our results a bit depending on how we wanna do this. I'm simply going to replace this with the value of 51. There may be some people who have several more, more years of coding experience than 51 years, but I can't imagine that it would be that many people who have, you know, coded many years greater than 50. So I'm just gonna fill this in with 51, but like I said, depending on what we pick here, it could affect our results slightly, but not by a lot in this case. So I'm just going to grab this same replace value here, and instead, I want to replace more than 50 years, and I'm gonna replace that with a value of 51. So now let me go ahead and run this. And if we want to look at these unique values again, then we could look at these, and now it doesn't look like we have any strings in here. But we can see here that this is still a dtype of object, which means that it's not actually reading this in as floats. So if we scroll back up here a bit oh, actually, I think I overwrote that line. Yes, I did. So let's just try that again. So what I wanna do here is I want to convert this to a float, and this is what, gave us an error before because we had these strings in here and it didn't know how to convert these to a float. But now, we should just be able to see say, okay, I want to convert that as type, set that to a float. So let's run that, and we didn't get an error this time, so that's good. And now we should be able to view the average numbers of or average number of years of coding experience of the developers who filled out this survey. So to do this, I'm just going to say, okay, def data frame, access that column, and grab the mean of that column. So if I run this, then we can see that now we get that average back. So the average that we got here was about 11 and a half years of coding experience, as the average years for developers who answered this survey. And now you can do other analysis on this as well. So for example, if we wanted to see the median, then I could run that, and the median comes back",
        "video_title": "Python Pandas Tutorial (Part 9): Cleaning Data - Casting Datatypes and Handling Missing Values",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_13",
        "text": "out this survey. So to do this, I'm just going to say, okay, def data frame, access that column, and grab the mean of that column. So if I run this, then we can see that now we get that average back. So the average that we got here was about 11 and a half years of coding experience, as the average years for developers who answered this survey. And now you can do other analysis on this as well. So for example, if we wanted to see the median, then I could run that, and the median comes back as 9 years of coding experience. So hopefully, that real world example helped explain why it's important to know how to cast these values and understand understand what's going on there. There's always going to be data that is messy or not in the format that we want it in, so knowing how to handle these missing values and cast these values to different data types is really going to be essential, when working with Pandas. Okay. So before we end here, I'd like to thank the sponsor of this video and mention why I really enjoy their tutorials, and that is Brilliant. So in this series, we've been learning about Pandas and how to analyze data in Python, and Brilliant would be an excellent way to supplement what you learn here with their hands on courses. They have some excellent courses and lessons that do a deep dive on how to think about and analyze data correctly. For data analysis fundamentals, I would really recommend checking out their statistics course, which shows you how to analyze graphs and determine significance in the data. And I would also recommend their machine learning course, which takes data analysis to a new level where you'll learn about the techniques being used that allow machines to make decisions where there's just too many variables for a human to consider. So to support my channel and learn more about Brilliant, you can go to brilliant.org forward slash c m s to sign up for free. And also, the first 200 people that go to that link will get 20% off the annual premium subscription, and you can find that link in the description section below. Again, that's brilliant.orgforward/cms. Okay. So I think that's gonna do it for this Pandas video. I hope you feel like you got a good idea for how to handle these missing values and cast our data to different data types, so that we can do exactly what we want to do in terms of analyzing our data. Now in the next video, we're going to be learning how to work with dates and time series data. Now, I've been using the Stack Overflow survey data for this entire series, because I love being able to show you all real world examples of how these concepts apply. But our Stack Overflow survey data doesn't have any time series data that we can actually work with, so I'm going to be using a",
        "video_title": "Python Pandas Tutorial (Part 9): Cleaning Data - Casting Datatypes and Handling Missing Values",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_14",
        "text": "values and cast our data to different data types, so that we can do exactly what we want to do in terms of analyzing our data. Now in the next video, we're going to be learning how to work with dates and time series data. Now, I've been using the Stack Overflow survey data for this entire series, because I love being able to show you all real world examples of how these concepts apply. But our Stack Overflow survey data doesn't have any time series data that we can actually work with, so I'm going to be using a different dataset for the next video. And I still haven't narrowed down exactly what I'll be using, but I'll be sure that it allows us to do some analysis on some real world data like we've been doing. So maybe we'll use time series data to, you know, analyze cryptocurrency rates over time or something like that. But if anyone has any questions about what we covered in this video, then feel free to ask in the comments section below, and I'll do my best to answer those. And if you enjoy these tutorials and would like to support them, then there are several ways you can do that. The easiest way is to simply like the video and give it a thumbs up, and also it's a huge help to share these videos with anyone who you think would find them useful. And if you have the means, you can contribute through Patreon, and there's a link to that page in the description section below. Be sure to subscribe for future videos, and thank you all for watching.",
        "video_title": "Python Pandas Tutorial (Part 9): Cleaning Data - Casting Datatypes and Handling Missing Values",
        "video_id": "Unknown ID"
    }
]