[
    {
        "chunk_id": "chunk_0",
        "text": "Hey there. How's it going everybody? In this video, we're gonna be learning how to read and write data to different sources. So we'll learn how to read and write data using CSV files, Excel files, JSON, and also SQL databases. Now in this series so far, we've been reading, data from CSV files, but in data science there are so many different ways for data to be stored. So by the end of this video, you should be able to get your data to and from Pandas, no matter what data format you're using. Now if you're watching this video because you're looking for how to read and write a specific file format, then I'll be sure to add timestamps in the description section below to where, we read and write from each different format. Now I would like to mention that we do have a sponsor for this series videos, and that is Brilliant. So I really want to thank Brilliant for sponsoring this series, and it would be great if you all could check them out using the link in the description section below and support the sponsors. And I'll talk more about their services in just a bit. So with that said, let's go ahead and get started. Okay. So first, let's look at CSV files since we've already been using these throughout the series. We should already be familiar with reading data in from CSV since that's what we've been doing so far. But in case this is the first video of the series that you're watching, let's go over this one more time, and then we'll also learn how to write to a CSV file as well. So up here towards the top of my notebook here, we can see that I'm reading in this CSV file. And this CSV file is within a data folder that is in the same location as this Jupyter notebook on the file system. Now if you have a CSV file loaded elsewhere on the system, then you'll need to pass in the full path to that file instead of just this relative location that we have here. And we can see that we have different arguments that we can pass in when reading our CSV files. So in this example, I'm automatically setting the index, to this respondent column here, which is the respondent ID for each person who answered this survey. And when I read in the CSV, we can see that it sets this data frame, equal to the data, and we can print this data out down here at the bottom. So that is the read CSV method, and it allows us to pull data in to Pandas. Now let's learn how to write this data back to a CSV. So maybe you're going to make some changes and some different analysis here to your DataFrame, and then we want to export this back to our file system for later use or so that we can share it with someone else or something like that. So",
        "video_title": "Python Pandas Tutorial (Part 11): Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_1",
        "text": "we can see that it sets this data frame, equal to the data, and we can print this data out down here at the bottom. So that is the read CSV method, and it allows us to pull data in to Pandas. Now let's learn how to write this data back to a CSV. So maybe you're going to make some changes and some different analysis here to your DataFrame, and then we want to export this back to our file system for later use or so that we can share it with someone else or something like that. So for example, let's filter down for a specific location in this survey. You know, maybe you're doing some analysis for your specific country, and you just want to see the survey results from that location. We've seen this in previous videos, but if we want to filter, then we can simply say I'll create a filter here, and just say that I want the, country here, and I'll grab if the country survey results from India. So now I'm going to create a new DataFrame here. I'll call this IndiaDF, and do a df.loc and pass in that filter. So now if I do an India df dot head to take a look at the beginning of this new DataFrame, if we look over here in the Country column, then we can see that all of these countries here are now set to India. So now let's say that we want to export this new filter DataFrame to a CSV file. So to do this, we can use the to CSV method. So we can say, I'll just say india_df, which is our DataFrame.2_csv. And now I'm just gonna pass it into that same location in that data directory, and then I'll just call this modified. Csv. So if I run this, we can see that we don't get any errors. And now if I go back and look at my, file system here, then I have this modified ds dot CSV. So if I click on this, then we can see that this is, you know, a little bunched together since it's a CSV file, a raw CSV file that we're looking at. But we can see that we have all of our column names here, and then the 2nd row should be the first result. And I can see here that we have India for that country. If I look at the second result, we can see we have India again, and India again down here most likely. I can't see it, but, you know, we can just assume that it's there. It's looking good. Oh, and actually, there it is right there. So we can see that, we did actually export that data frame, where we filtered that down to a new CSV file. Okay. So that was easy enough. So now let's look at how to read and write to some other formats. So one thing that you might run into is a tab delimited file. These are",
        "video_title": "Python Pandas Tutorial (Part 11): Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_2",
        "text": "at the second result, we can see we have India again, and India again down here most likely. I can't see it, but, you know, we can just assume that it's there. It's looking good. Oh, and actually, there it is right there. So we can see that, we did actually export that data frame, where we filtered that down to a new CSV file. Okay. So that was easy enough. So now let's look at how to read and write to some other formats. So one thing that you might run into is a tab delimited file. These are almost exactly the same thing as CSV files, but instead of your data being separated by a comma, the data is instead separated by tabs. So to do this in Pandas, we're still going to use the same CSV methods that we've already seen, but we're going to pass in a custom separator. So we can write to a tab delimited file, just by changing the file extension here to dottsv, and I'm also going to specify a separator argument. So I'm going to say sep, s e p, and then you want to pass in your separator. Now you can pass in anything here if you want, you know, a, a file that is separated by hashes or anything, but commas and tabs are probably the most common. So I'm going to put a backslasht there because that's how we specify tabs in Python. And now if I run this cell, I'm going to go back to our data directory here, we can see that now we have this modified dottsv. If I click on that, then we can see that now this looks almost exactly the same as the comma separated file, but now we have tabs here, instead of commas. Now if you're reading in tab CSV files, then all you need to do is take this sep equal to, backslasht, and you can just add that as an argument up here to read CSV. So it's basically the same thing. Okay. So now let's move on to some other file formats. Now a very popular file format when working with this kind of data is Excel. Now if we want to write to Excel, then we're going to need to PIP install a couple of packages. So I have my terminal open with, the current environment that I am using. This is my Jupyter notebook running here. Let me grab my other terminal. So I have the same environment that I'm using within Jupyter. You want to be sure, that you're using that same environment so that, you're PIP installing in the right location. And now we're going to install a couple of packages. So first I'm going to say pip install, and this is xlwt. So xlwt will write to an older XLS Excel format, But if you want to write to a newer XLSX Excel format, then we'll also need to install OpenPyXL. And you can PIP install multiple packages bus just by, listing them all",
        "video_title": "Python Pandas Tutorial (Part 11): Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_3",
        "text": "me grab my other terminal. So I have the same environment that I'm using within Jupyter. You want to be sure, that you're using that same environment so that, you're PIP installing in the right location. And now we're going to install a couple of packages. So first I'm going to say pip install, and this is xlwt. So xlwt will write to an older XLS Excel format, But if you want to write to a newer XLSX Excel format, then we'll also need to install OpenPyXL. And you can PIP install multiple packages bus just by, listing them all right here. And finally, we want if we want to read Excel files, then we can install the XLRD package. So I think that is, the 3 packages we're gonna need in order to work with Excel files here. So I'll go ahead and install all of those and let those finish. And once those are installed, let's go back to our notebook, and now let's try to write to an Excel file. So to write to an Excel file, I'm just going to write the same modified, data frame that we have here, and we are going to use the 2 underscore Excel method. And this is just as easy as passing in, let's see. I'll save it in that data folder again. I'll call this modified dot xlsx. So I'm gonna write to the, newer Excel format. So if I run this, then it might take a second here for this to work because it's actually creating this Excel file on the back end. So let's let this finish, and we can tell it's finished when this turns from an asterisk to a number here. Okay. So once that's finished, let's flip over to our data folder here, and we can see that we do have that .xlsx file. Now this likely won't open up in Jupyter because this is an Excel file. We can see here, that we can't open this up in the browser. We actually need Excel. So let me open up my finder window here. I have this open down here, and I am within this data folder, and we can see that we have our modified dot xlsx file here. Now I don't actually have Excel on this machine. I have Numbers, so I'm going to open this up in Numbers. It should basically be the same on Windows, but you can just open it up with Excel. Now again, this might take a second to open up because we do still have a lot of rows here, in this data. Okay. So we've got this opened up, in Excel. Again, I'm on Numbers because I'm on a Mac and I have Excel installed, but it should open up fine in Excel as well. Let me zoom in a little bit here. So we can see and we can format these if, if we need to. So for example, we can change the column sizes here so that all these fit in. But we can",
        "video_title": "Python Pandas Tutorial (Part 11): Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_4",
        "text": "up with Excel. Now again, this might take a second to open up because we do still have a lot of rows here, in this data. Okay. So we've got this opened up, in Excel. Again, I'm on Numbers because I'm on a Mac and I have Excel installed, but it should open up fine in Excel as well. Let me zoom in a little bit here. So we can see and we can format these if, if we need to. So for example, we can change the column sizes here so that all these fit in. But we can see here that we have our respondent IDs. If I look over at Country, we can see that it did export, the filtered DataFrame that we were hoping to export. So everything looks good here. Now there are also some more advanced things that we can do with Excel as well. If you're familiar with Excel, then you might know that we have the concept of different sheets where we can have multiple spreadsheets in 1 Excel file. And if you want to read or write to a specific sheet, then you can pass in a sheet argument, to these methods. Actually, I'm trying to scroll over to my notebook here. Let me scroll down here to the bottom. So like I was saying, if you want to read or write to a specific sheet, then you can pass in a sheet argument to these methods, and there's also a way to start from different columns and rows as well. But I'm not going to go into all these little details here. If you Google this method name to Excel, then you can find the arguments that you can pass in and all the additional details in the documentation. So for now, let's go ahead and move on and see how that we can, read in that same Excel file that we just created and make sure that this works. Now, by default, it's going to load in with a default index, just like, when we read a CSV file. So we'll have to specify that we want our index column to be that respondent column. So to do that, I'm just going to call this test since we're going to be creating a new data frame here from that Excel file that we just created. And we're going to use the read underscore Excel, method here. Now we just want to pass in the location, and I'll just go ahead and copy this here. So that is modified dot xlsx on my machine. And now I'm going to set that index column equal to and that was respondent. On your data, that might be different, but I want my index column to be equal to that respondent. So I'm going to run that cell and load that in, and then I'm going to look at that test data frame. Now before I run this, I'm gonna make sure that this, finishes processing here and that this asterisk goes away. Again, it",
        "video_title": "Python Pandas Tutorial (Part 11): Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_5",
        "text": "pass in the location, and I'll just go ahead and copy this here. So that is modified dot xlsx on my machine. And now I'm going to set that index column equal to and that was respondent. On your data, that might be different, but I want my index column to be equal to that respondent. So I'm going to run that cell and load that in, and then I'm going to look at that test data frame. Now before I run this, I'm gonna make sure that this, finishes processing here and that this asterisk goes away. Again, it can take some time because it's actually, you know, loading in that data from Excel now, which is a little more tricky than loading it in from a CSV. So now if we look at that test, data frame let me just look at the head here instead of looking at the whole thing. If I look at the head, then we can see that we, have the same data frame here, that we had up here, so that was exported to Excel and imported correctly. Okay. So now let's cover some other popular file formats. Now, JSON is also really popular, for this kind of data. So let's take a look at that. First, let's write our modified DataFrame to a JSON file. Now, for writing to a JSON file, then we can use the toJSON method. So you're probably starting to see a pattern here. These method names are very straightforward. Now this one is a bit different since there are some different orientations that we can use for JSON. So just by, using the default arguments, I can just say, so that was India DF dot 2_ JSON, And now I'll pass in a file location here, but instead of an Excel file, we want a JSON file. Now, I'm just going to use the default arguments for now, and then I'll show you, how we can change this up a bit. So if I run this, we can see that ran very quickly. If I go back to my data folder here, then now we have this JSON file. If I look within here okay. That took just a second to open up on my machine. Again, we do have a lot of data in here. But if we look in here, then we can see that this is very dictionary like. So we have, a main branch key here, and then the value for that key are all of the responses just for that column. And if I was to scroll down here, then I would be able to find the other keys and the other responses as well. So by default, this is a dictionary like JSON. Now there are also, different ways that we can write JSON files. Again, I'm not going to go into every single little detail here, but let's say that we wanted this JSON, to be list like instead of dictionary like, which is how it is by default. So to",
        "video_title": "Python Pandas Tutorial (Part 11): Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_6",
        "text": "key here, and then the value for that key are all of the responses just for that column. And if I was to scroll down here, then I would be able to find the other keys and the other responses as well. So by default, this is a dictionary like JSON. Now there are also, different ways that we can write JSON files. Again, I'm not going to go into every single little detail here, but let's say that we wanted this JSON, to be list like instead of dictionary like, which is how it is by default. So to do this, we can change the orient argument. So instead, let's add 1 here to our arguments, and I'm going to say orient is equal to, and if we pass in records and lines equal to true, then this will now make this records like, which is, list like, and this lines equal to true. Let me spell that right. We'll just make each of these on a new line, so it might be a little bit easier to read. Now if you want to see the exact arguments that you can pass into Orient, then again just look up Pandas to JSON method. And it'll take you to the documentation with all the different things that you can pass in here. So let me run this, and now let's go back and reload our JSON file to see how this looks. And now what we have here is more list like. So before, we had, a single dictionary where the values were a list of all of the responses, but now we have one response at a time. So we have the main branch, and then, so this is this first one here. If I scroll down, we can see that this is the second response. This is actually the entire first response. So we have the main branch, and then that answer, and then open source, then that answer, and then so on. And we can see here that for the country, we have India. And each response within this survey is actually on a different line. So that's a little bit different than how it was before, but there's just different ways that we can export these JSON files depending on your needs. Okay. So now that we've written our data to JSON files, now let's also read this JSON file, so that we can make sure, that we know how that's done as well. Now, since we wrote the JSON file with these different arguments here, then we need to use those same arguments when we read the data in as well. So if you're reading in JSON files and have any issues, then you might need to play around with the different arguments to fit the data that you're trying to read in. So in this case, I'm just going to copy this whole line here and I'm going to say test is equal to, and actually let me just grab this part, And I'll say pd.read_json.",
        "video_title": "Python Pandas Tutorial (Part 11): Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_7",
        "text": "know how that's done as well. Now, since we wrote the JSON file with these different arguments here, then we need to use those same arguments when we read the data in as well. So if you're reading in JSON files and have any issues, then you might need to play around with the different arguments to fit the data that you're trying to read in. So in this case, I'm just going to copy this whole line here and I'm going to say test is equal to, and actually let me just grab this part, And I'll say pd.read_json. And now I'll pass in all those arguments here. So we are reading the JSON file from this location. We know that the orient is list like instead of dictionary like, and that all of these are on new lines. And again, depending on your JSON data, you might need to go in and change these around, depending on how your data looks. So if I run this, then let's see if we have the same data that we exported before. And it seems like we do. This, looks exactly, like it did whenever we exported this data. Okay. So now the last, file format that we're going to look at, let's learn how we can, read and write data from SQL databases. Now, this is probably the most complicated simply because you have to have the database set up and all of that good stuff, but for the sake of this video, I'm going to assume that you already have a database with the correct credentials to log in to that database. So I have a Postgres database set up on my machine that we'll be reading and writing to. So first, let's see how we would connect to this database. Now just like with Excel, we're going to need to install a package to do this. So let me bring up my terminal here, and I'll close this, Numbers file here. Let's see. Let me try to quit out of this. Actually, I'll just minimize it. It's having trouble shutting down. Okay. So let me go back to the terminal that I have opened to where I can install some different packages, and that's my Jupyter notebook. Where is my other terminal? Here we go. Okay. So to connect to our database, we're going to want to install SQLAlchemy, and this is a very popular ORM for Python that allows us to more easily work with databases. If you don't know what an ORM is, it stands for Object Relational Mapper, and it's just a way for us to use Python objects in order to connect to a database. I plan on doing a complete video or a complete series on SQLAlchemy in the future. But for now, let's go ahead and just install this. So this is pip install SQL Alchemy, and I'll install that. And depending on the database that you're using, you might not need to do anything else here. So for example, if you're using SQLite",
        "video_title": "Python Pandas Tutorial (Part 11): Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_8",
        "text": "us to more easily work with databases. If you don't know what an ORM is, it stands for Object Relational Mapper, and it's just a way for us to use Python objects in order to connect to a database. I plan on doing a complete video or a complete series on SQLAlchemy in the future. But for now, let's go ahead and just install this. So this is pip install SQL Alchemy, and I'll install that. And depending on the database that you're using, you might not need to do anything else here. So for example, if you're using SQLite or something like that. But since I'm using a Postgres database for this tutorial, I also need to install the Psycho PG 2 package that allows us to work with Postgres. I'm not sure if that's actually how you say that package name, but that's what I've always called it. So pip install, and to install, this package to work with, Postgres, it's psychopg2dashbinary. So I'll install that. And with those packages installed, let's go back to our notebook and see if we can connect to this database using SQLAlchemy. So first, we're going to want to import everything that we need. So from SQLAlchemy, I'm going to want to import their Create Engine, and this will allow us to, connect to the database. Now I'm also going to want to import, Psycho PG 2. So let me run this cell. And now that those are imported, we should be able to create the engine, which is basically our database connection. And again, I'm going to assume that you've already created this database and have a username and password. So to create this, I can say engine is equal to, and use that create engine function that we just imported from SQLAlchemy, and now we need our Postgres, connection string. Now if you don't know how to make Postgres connect or, connection strings, then, you know, they have this available on the, SQLAlchemy site as well. Let me make sure I spelled this correctly. That is PostgresQL, and then we want to pass in the username and password, for our database. Now for my case, I just made a user of DB user and a password of DB pass. Now another thing here, that I'd like to mention is that, you probably shouldn't put credentials within code like this. I'll leave a link in the description section below where I show how, in Python you should use, you know, something like environment variables or a config file to hide this information. But for the sake of this tutorial, I'm just gonna put it directly in here. But if you're doing this in, in production code, I would highly recommend using environment variables so that, you know, you don't expose your username and passwords within your code base. Okay. So there we have our username and password, and now the database that we want to connect to. So this is on localhost, this is on my local machine. It's running on port 5,432,",
        "video_title": "Python Pandas Tutorial (Part 11): Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_9",
        "text": "in Python you should use, you know, something like environment variables or a config file to hide this information. But for the sake of this tutorial, I'm just gonna put it directly in here. But if you're doing this in, in production code, I would highly recommend using environment variables so that, you know, you don't expose your username and passwords within your code base. Okay. So there we have our username and password, and now the database that we want to connect to. So this is on localhost, this is on my local machine. It's running on port 5,432, and now the name of the database. Now I have pgAdmin open here, where I can see my databases, and we can see that I've just created an empty database here called sample_db. So that is the database that I'm going to connect to. Okay. So if I typed everything correctly here, then I should be able to, get a connection to that database. So now let's try to write our modified DataFrame to a table in this database. And this table doesn't need to currently exist. By default, it will create this table for us. If it does already exist, then we'll need to add another argument to handle that, but we'll see that in just a second. So to do this, I can just say India underscore d f, which is the data frame we want to export, then this is 2 underscore SQL, and now the table that we want to write this data to. I'm just going to call this sample underscore table. Now again, this doesn't currently exist, but it should create it. And now we need to pass in our database connection here. I called mine engine, so let's pass that in. And if I run this, let's see if this works. Okay. So we didn't get any errors whenever I read that or whenever I wrote that, but now let's, go back to my pgAdmin here, and let's see if I can see this table. So first I'm just going to right click and refresh. I like to do that anytime, I've made any changes. We can see there here that we have a sample table down here. I'm going to right click on that and go to View and Edit Data, and look at all the rows here. And we can see it does look like, this worked. I know that this is probably a little difficult to see, on my screen here, but we have, all of our data written here into the database. Okay. So that's good that we were able to, get this from Pandas into SQL. But now what if we updated our data and wanted to rewrite that data, to this database? Let's go back to our notebook and see what this would look like. Now, if I try to run this same line again where we export this to SQL, then we're actually going to get an error because this table already exists. If you want to, write",
        "video_title": "Python Pandas Tutorial (Part 11): Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_10",
        "text": "a little difficult to see, on my screen here, but we have, all of our data written here into the database. Okay. So that's good that we were able to, get this from Pandas into SQL. But now what if we updated our data and wanted to rewrite that data, to this database? Let's go back to our notebook and see what this would look like. Now, if I try to run this same line again where we export this to SQL, then we're actually going to get an error because this table already exists. If you want to, write over a table, then we can add in an additional argument. And the argument that we want to add in is called if underscore exists equals and now what we want to do if this table already exists. Now in my case, I'm just going to replace that table with our new data, but there are also other options as well. We could have it throw an error. We could which is what it does by default. We could also append data to a table. So if you're doing like a, daily script where you're analyzing information, then you can just append that daily data to your existing table. But for this example, I'm just going to, have this replace the table. So let's run this, and once this, is finished processing, then I will go back to pgAdmin. Now again, let me come up here and refresh this and dig back down into the database. And let me close this view here and let's see if we still have this data. Okay. So we can see that this worked. We were able to, rerun that command and it just replaced that data that was in that existing table, with our new data. In this case, it was the same data, but, that's how you would do that. Okay. So lastly, now that we've seen how to add our data to a database, now let's see how we can read in this same data using SQL. Now if you skip to this part of the video using the timestamps in the description sections below, then please go back to when we wrote data to our database and see how I set up this database connection here, because we're going to reuse that same connection to read in our data. Okay. So this is pretty simple now that we actually have this database connection set up. To do this, we can just say, I'll call this SQL underscore DF, and we will just say pd.read_sql, and now we wanna pass in the table that we're going to read from, and that was sampletable. And now pass in our database connection. My connection here, I called engine. And also, I'm also going to, pass in an index column just like we did when we read in our CSV. So I'll say index column is equal to, and that is going to be this respondent row right here. For your data, that might be different.",
        "video_title": "Python Pandas Tutorial (Part 11): Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_11",
        "text": "connection set up. To do this, we can just say, I'll call this SQL underscore DF, and we will just say pd.read_sql, and now we wanna pass in the table that we're going to read from, and that was sampletable. And now pass in our database connection. My connection here, I called engine. And also, I'm also going to, pass in an index column just like we did when we read in our CSV. So I'll say index column is equal to, and that is going to be this respondent row right here. For your data, that might be different. So whatever you want to be your index, just pass it in there. If you want Pandas to just do a default index, then you can just leave this off entirely. Okay. So if I run this, then let's look at SQLDF. Head to make sure this worked, and we can see that that worked well. We still have, the same data frame here that we started off with, where we filter down these countries to just be the results from India. Now there might be instances where you don't want to load in an entire table, but you want to run a specific SQL query in order to load in our data. To do this, we can use the method read underscore SQL underscore query to run a specific SQL query. So let me just copy what I did here and paste this down here. And now instead of reading in, this entire table, I'm going to actually run a query here. I'll do read underscore SQL underscore query, and now instead of the table name here, I'm actually going to pass in a SQL query. Now I'm just going to, load in everything here. So I'll say select star from sample_table. And everything else here is going to be the same. We still have our database connection, and we still want our index column to be equal to respondent. So this is still going to grab all the rows, but if you wanted to customize this, then you could add in a where clause here, to filter this down. So let me run this, and now let's look at our, SQL DataFrame here, and we can see that that worked as well. So we loaded in this data using a SQL query instead of just reading in the entire table. So that can be especially useful, when you're working with large databases where you only want to load in specific data using a query. Okay. So we're just about finished up here, but let me show you one more tip before we wrap this up. So you may have seen people load in data using a URL instead of a specific file for some of the methods that we've looked at before, and we can do that. All you need to do, is you need to be sure that you're using the correct method for whatever form of data is on the URL. So for example, in my Flask and",
        "video_title": "Python Pandas Tutorial (Part 11): Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_12",
        "text": "large databases where you only want to load in specific data using a query. Okay. So we're just about finished up here, but let me show you one more tip before we wrap this up. So you may have seen people load in data using a URL instead of a specific file for some of the methods that we've looked at before, and we can do that. All you need to do, is you need to be sure that you're using the correct method for whatever form of data is on the URL. So for example, in my Flask and Django series, I created a JSON file of some sample posts for the website that we were creating in that series, and I have that JSON file on my GitHub page. Now, if I wanted to bring that into Pandas, then I could simply use the read JSON method and then pass in that URL. I wouldn't actually have to download that JSON first and then pass it in that way. So I have this open here. If you didn't know, on GitHub, you can look at the raw files. So we can see that this is a long URL here, but I will have this code posted in the description section below if you'd like to follow along. So I'm just going to copy this URL and this isn't on my file system. And now, let's see if we can just load this in. So I'm going to call this post underscore d f and I'll set this equal to pd.read_json since this is JSON on the URL. If it was CSV, then you'd want to use read CSV and so on. So now I can just paste in that URL there and now let's just run that cell and we can see that we didn't get any errors. So let me now look at the head of our DataFrame here. And we can see, that I do have my sample posts here. These are the sample posts, that I used on, that website series. So depending on the data in that URL, you should be able to use the methods that we've seen to load in data from a URL just like we did here. Now before we end here, I would like to thank the sponsor of this video, and that is Brilliant. I really enjoy the tutorials that Brilliant provides and would definitely recommend checking them out. Brilliant is a problem solving website that helps you understand underlying concepts by actively working through guided lessons. And Brilliant would be an excellent way to supplement what you learn here with their hands on courses. They have some excellent courses and lessons on Data Science that do a deep dive on how to think about and analyze data correctly. So if you're watching my Panda series because you're getting into the Data Science field, then I would highly recommend also checking out Brilliant and seeing what other Data Science skills you can learn. They even use Python in their statistics",
        "video_title": "Python Pandas Tutorial (Part 11): Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_13",
        "text": "is a problem solving website that helps you understand underlying concepts by actively working through guided lessons. And Brilliant would be an excellent way to supplement what you learn here with their hands on courses. They have some excellent courses and lessons on Data Science that do a deep dive on how to think about and analyze data correctly. So if you're watching my Panda series because you're getting into the Data Science field, then I would highly recommend also checking out Brilliant and seeing what other Data Science skills you can learn. They even use Python in their statistics course and will quiz you on how to correctly analyze the data within the language. Their guided lessons will challenge you, but you'll also have the ability to get hints or even solutions if you need them. It's really tailored towards understanding the material. So to support my channel and learn more about Brilliant, you can go to brilliant.org forward slash cms to sign up for free. And also, the first 200 people to go to that link will get 20% off the annual premium subscription, and you can find that link in the description section below. Again, that's brilliant.orgforward/cms. Okay. So I think that's gonna do it for this Pandas video. I hope you feel like you got a good idea for how to read and write data from multiple different sources. What we covered here should cover the vast majority of file formats that you are going to be seeing and using in the data science field. Now I'm probably going to take a break from this Pandas series after this video and do a few one off videos that I've been wanting to cover. But I know that there are a lot of topics in Pandas left to cover, and I will get around to those more advanced topics in future videos. But in the meantime, if you'd like a good source for learning Pandas, then I would highly recommend checking out the channel Data School. That's run by Kevin Markham, and he's done the Pandas tutorials at PyCon for several years now. Now he didn't, you know, ask me to suggest his channel or anything like that. I just think that, he does a good job. And his channel is actually completely devoted to Pandas and data science, so he's already covered some of the more advanced topics that I do plan to cover in future videos. But if anyone has any questions about what we covered in this video, then feel free to ask in the comments section below, and I'll do my best to answer those. And if you enjoy these tutorials and would like to support them, then there are several ways you can do that. The easiest way is to simply like the video and give it a thumbs up, and also it's a huge help to share these videos with anyone who you think would find them useful. If you have the means, you can contribute through Patreon, and there's a link to",
        "video_title": "Python Pandas Tutorial (Part 11): Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc",
        "video_id": "Unknown ID"
    },
    {
        "chunk_id": "chunk_14",
        "text": "if anyone has any questions about what we covered in this video, then feel free to ask in the comments section below, and I'll do my best to answer those. And if you enjoy these tutorials and would like to support them, then there are several ways you can do that. The easiest way is to simply like the video and give it a thumbs up, and also it's a huge help to share these videos with anyone who you think would find them useful. If you have the means, you can contribute through Patreon, and there's a link to that page in the description section below. Be sure to subscribe for future videos, and thank you all for watching.",
        "video_title": "Python Pandas Tutorial (Part 11): Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc",
        "video_id": "Unknown ID"
    }
]